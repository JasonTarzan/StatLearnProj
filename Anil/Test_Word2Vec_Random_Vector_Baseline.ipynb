{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Test Word2Vec Random Vector Baseline.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QuARbianQmO",
        "colab_type": "text"
      },
      "source": [
        "## Sense check: Use Random Vectors\n",
        "\n",
        "To give a sense of the improvement the Word2Vec vectors, which effectively embed words in a latent _semantic space_, give over a baseline, I repeat the exact same procedure using vectors ($\\in \\mathbb{R}^{300}$) whose elements are drawn from a random uniform distribution supported over $[-10, 10]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-6aYK3UnQmQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3cad762c-7ec4-4fae-a753-256b312a3786"
      },
      "source": [
        "# Imports and Set Options\n",
        "\n",
        "import csv  # for slang\n",
        "import os\n",
        "import re  # regex\n",
        "import string  # punct\n",
        "from pprint import pprint\n",
        "\n",
        "import gensim\n",
        "import keras\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "from IPython.display import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.corpus import stopwords  # stopwords\n",
        "from nltk.stem import PorterStemmer  # stemming\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import svm, tree\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
        "                              GradientBoostingClassifier,\n",
        "                              RandomForestClassifier, RandomForestRegressor,\n",
        "                              StackingClassifier)\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import (accuracy_score, auc, average_precision_score,\n",
        "                             brier_score_loss, classification_report,\n",
        "                             confusion_matrix, f1_score, fbeta_score,\n",
        "                             make_scorer, plot_precision_recall_curve,\n",
        "                             precision_recall_curve, precision_score,\n",
        "                             recall_score, roc_auc_score, roc_curve)\n",
        "from sklearn.model_selection import (GridSearchCV, KFold, RandomizedSearchCV,\n",
        "                                     cross_val_score, train_test_split)\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
        "from sklearn.svm import SVC  # \"Support vector classifier\"\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "# pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnMY6WZZnQmY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f9e49622-3cac-4c88-d05a-535b1182f864"
      },
      "source": [
        "print(os.getcwd())\n",
        "# os.listdir('..')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cKe5V-enQmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in cleaned data processed earlier\n",
        "\n",
        "tweets = pd.read_csv(\"https://github.com/anilkeshwani/StatLearnProj/raw/master/Anil/clean_tweets_no_stemming.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnNjJVUYnQmo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "a07bf9db-eb0a-4e6a-b290-175029bcef01"
      },
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFHKZKF8nQmw",
        "colab_type": "text"
      },
      "source": [
        "## Add clean text field containing only words known to pretrained Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQhXBcXjnQmy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "6915167f-5079-45e5-8a56-724be29a67ff"
      },
      "source": [
        "tweets[\"text_clean_known\"] = tweets.text_clean.apply(func=lambda tweet: ' '.join([word for word in tweet.split() \\\n",
        "                                                                               if word in wv.vocab]))\n",
        "print(f\"Count of text_clean_known entries which are null: {sum(tweets.text_clean_known.isnull())}\")\n",
        "print(f\"Count of text_clean_known entries which empty: \\\n",
        "# {sum(tweets.text_clean_known.apply(func=lambda x: x.strip() == ''))}\")\n",
        "\n",
        "# Remove both rows with either null or empty `text_clean_known` entries\n",
        "\n",
        "tweets = tweets.loc[(~tweets.text_clean_known.isnull()), :]\n",
        "tweets = tweets.loc[~tweets.text_clean_known.apply(func=lambda x: x.strip() == ''), ]\n",
        "\n",
        "print(\"After cleaning:\", end=\"\\n\")\n",
        "print(f\"Count of text_clean_known entries which are null: {sum(tweets.text_clean_known.isnull())}\")\n",
        "print(f\"Count of text_clean_known entries which empty: {sum(tweets.text_clean_known.apply(func=lambda x: x.strip() == ''))}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of text_clean_known entries which are null: 0\n",
            "Count of text_clean_known entries which empty: # 3\n",
            "After cleaning:\n",
            "Count of text_clean_known entries which are null: 0\n",
            "Count of text_clean_known entries which empty: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFA3l63UnQnA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "277da303-1cf7-42c4-be3b-cb9501b0d1c6"
      },
      "source": [
        "# Generate a dictionary containing all the words in the corpus of tweets\n",
        "\n",
        "# {k: v for k, v in \n",
        " \n",
        "vocab = set()\n",
        "\n",
        "for tweet in tweets.text_clean_known:\n",
        "  for word in tweet.split():\n",
        "    vocab.add(word.strip())\n",
        "\n",
        "len(vocab)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14230"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2p6kMiowJKB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dddc706c-50e6-469a-8a37-020b18896499"
      },
      "source": [
        "# Create random vector representations\n",
        "\n",
        "rv = {k: np.random.uniform(low=-10.0, high=10.0, size=300) for k in vocab}\n",
        "\n",
        "# Then we have our vectors infilled with random uniformly distributed elements\n",
        "\n",
        "len(rv[\"hello\"])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE_iCAYKwhIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RandomVectorizeTweet(tweet, rv=rv):\n",
        "    tweet_vector = np.zeros(shape=(300,), dtype=\"float32\")\n",
        "    n_vectorizable = 0\n",
        "    for word in tweet.split():\n",
        "        try:\n",
        "#             print(f\"Adding {word} to word representation\")\n",
        "            tweet_vector = np.add(tweet_vector, rv[word])\n",
        "            n_vectorizable += 1\n",
        "        except KeyError:\n",
        "            print(f\"Could not vectorize {word}\")\n",
        "    return (tweet_vector/n_vectorizable)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHR8_YqGw9HV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets[\"rv\"] = tweets.text_clean_known.apply(func=RandomVectorizeTweet)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prn2DQhs0ldi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f13a4933-bb78-458f-8f77-3d871e24fcc7"
      },
      "source": [
        "tweets.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>username</th>\n",
              "      <th>user_handle</th>\n",
              "      <th>date</th>\n",
              "      <th>retweets</th>\n",
              "      <th>favorites</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>text_clean_known</th>\n",
              "      <th>rv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-04-28</td>\n",
              "      <td>11</td>\n",
              "      <td>22</td>\n",
              "      <td>Economic recovery and national climate pledges...</td>\n",
              "      <td>0</td>\n",
              "      <td>economic recovery national climate pledges mus...</td>\n",
              "      <td>economic recovery national climate pledges mus...</td>\n",
              "      <td>[-1.6829878142677979, 0.8212961599045651, 0.30...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-04-22</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>In this difficult time, it’s hard to connect w...</td>\n",
              "      <td>0</td>\n",
              "      <td>difficult time hard connect natural world eart...</td>\n",
              "      <td>difficult time hard connect natural world eart...</td>\n",
              "      <td>[0.6293775065181396, 0.14288798573284223, 1.28...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-04-01</td>\n",
              "      <td>43</td>\n",
              "      <td>69</td>\n",
              "      <td>The decision to postpone # COP26, is unavoidab...</td>\n",
              "      <td>0</td>\n",
              "      <td>decision postpone cop unavoidable collective p...</td>\n",
              "      <td>decision postpone cop unavoidable collective p...</td>\n",
              "      <td>[-1.0888818806897693, -1.0935972221220953, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-03-30</td>\n",
              "      <td>24</td>\n",
              "      <td>30</td>\n",
              "      <td>Japan - the world’s fifth largest emitter of g...</td>\n",
              "      <td>0</td>\n",
              "      <td>japan worlds fifth largest emitter greenhouse ...</td>\n",
              "      <td>japan worlds fifth largest emitter greenhouse ...</td>\n",
              "      <td>[1.3617367872821422, -1.3378618822394823, 0.32...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-03-30</td>\n",
              "      <td>22</td>\n",
              "      <td>40</td>\n",
              "      <td>How can countries include # NatureBasedSolutio...</td>\n",
              "      <td>0</td>\n",
              "      <td>countries include naturebasedsolutions climate...</td>\n",
              "      <td>countries include climate plans new guidance o...</td>\n",
              "      <td>[0.11711354888495744, 1.3867587750494383, 4.59...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1              username user_handle        date retweets favorites                                               text  label                                         text_clean                                   text_clean_known                                                 rv\n",
              "0           0             0  WWF Climate & Energy  climateWWF  2020-04-28       11        22  Economic recovery and national climate pledges...      0  economic recovery national climate pledges mus...  economic recovery national climate pledges mus...  [-1.6829878142677979, 0.8212961599045651, 0.30...\n",
              "1           1             1  WWF Climate & Energy  climateWWF  2020-04-22        6        16  In this difficult time, it’s hard to connect w...      0  difficult time hard connect natural world eart...  difficult time hard connect natural world eart...  [0.6293775065181396, 0.14288798573284223, 1.28...\n",
              "2           2             2  WWF Climate & Energy  climateWWF  2020-04-01       43        69  The decision to postpone # COP26, is unavoidab...      0  decision postpone cop unavoidable collective p...  decision postpone cop unavoidable collective p...  [-1.0888818806897693, -1.0935972221220953, 0.0...\n",
              "3           3             3  WWF Climate & Energy  climateWWF  2020-03-30       24        30  Japan - the world’s fifth largest emitter of g...      0  japan worlds fifth largest emitter greenhouse ...  japan worlds fifth largest emitter greenhouse ...  [1.3617367872821422, -1.3378618822394823, 0.32...\n",
              "4           4             4  WWF Climate & Energy  climateWWF  2020-03-30       22        40  How can countries include # NatureBasedSolutio...      0  countries include naturebasedsolutions climate...  countries include climate plans new guidance o...  [0.11711354888495744, 1.3867587750494383, 4.59..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm0PHu8N0me7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rv_train, rv_test, Y_train, Y_test = train_test_split(np.array(tweets.rv.tolist()), tweets.label, \n",
        "                                                    test_size=0.2, random_state=17, \n",
        "                                                    shuffle=True) # explicit default"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej_6-zYz0uGx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "c22b931e-a80b-4324-e037-5339cf664878"
      },
      "source": [
        "rf_clf = RandomForestClassifier(oob_score=True)\n",
        "\n",
        "rf_clf.fit(rv_train, Y_train)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=True, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmQnaOuk0_7N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "0035d5c9-a6f2-427a-c748-4d894b508cbc"
      },
      "source": [
        "rf_clf_cv_score = cross_val_score(rf_clf, rv_train, Y_train)\n",
        "\n",
        "print(f\"Training Set Accuracy: {rf_clf.score(rv_train, Y_train)}\")\n",
        "print(f\"Out-of-Bag Score: {rf_clf.oob_score_}\")\n",
        "\n",
        "print(f\"Cross-validated accuracy : {rf_clf_cv_score}\") \n",
        "print(f\"Mean CV accuracy : {np.round(np.mean(rf_clf_cv_score), 3)}\")\n",
        "\n",
        "print(f\"Test set score : {rf_clf.score(rv_test, Y_test)}\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set Accuracy: 0.9997917245209664\n",
            "Out-of-Bag Score: 0.7706192724243266\n",
            "Cross-validated accuracy : [0.7965984  0.79451579 0.7764665  0.78028462 0.77083333]\n",
            "Mean CV accuracy : 0.784\n",
            "Test set score : 0.7875590113857261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To6HN7w31ov2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up Hyperparameter Search Space\n",
        "\n",
        "param_grid = {\n",
        "    'bootstrap': [True],\n",
        "    'max_depth': [80, 100, None],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'min_samples_leaf': [1, 3, 5],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'n_estimators': [50, 100, 300]\n",
        "}\n",
        "\n",
        "# Set Cross-Validation Process\n",
        "\n",
        "kfcv = KFold(n_splits=5, shuffle=True, random_state=101)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT58JmSm1rfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate the grid search model\n",
        "rf_grid_search = GridSearchCV(estimator = rf_clf, param_grid = param_grid, \n",
        "                              cv = kfcv, n_jobs = -1, verbose = 2)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkpF2Tsa18mP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "outputId": "52ea3e34-c880-4e69-a656-a0699d46cef6"
      },
      "source": [
        "rf_grid_search.fit(rv_train, Y_train)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 12.0min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 48.5min\n",
            "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed: 98.7min\n",
            "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed: 170.9min\n",
            "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed: 203.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=KFold(n_splits=5, random_state=101, shuffle=True),\n",
              "             error_score=nan,\n",
              "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                              class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='auto',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              max_samples=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_f...\n",
              "                                              n_estimators=100, n_jobs=None,\n",
              "                                              oob_score=True, random_state=None,\n",
              "                                              verbose=0, warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'bootstrap': [True], 'max_depth': [80, 100, None],\n",
              "                         'max_features': ['sqrt', 'log2'],\n",
              "                         'min_samples_leaf': [1, 3, 5],\n",
              "                         'min_samples_split': [2, 5, 10],\n",
              "                         'n_estimators': [50, 100, 300]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytgrBzze1-jv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "c798cad4-b563-4a84-b803-559896264dcb"
      },
      "source": [
        "# Best Score\n",
        "\n",
        "print(f\"Best Score: {rf_grid_search.best_score_}\", end=\"\\n\"*2)\n",
        "\n",
        "# Best Parameters\n",
        "\n",
        "print(\"Best parameters:\")\n",
        "for k, v in rf_grid_search.best_params_.items():\n",
        "    print(str(k) + \": \" + str(v))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Score: 0.7926263305565198\n",
            "\n",
            "Best parameters:\n",
            "bootstrap: True\n",
            "max_depth: None\n",
            "max_features: sqrt\n",
            "min_samples_leaf: 1\n",
            "min_samples_split: 2\n",
            "n_estimators: 300\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}