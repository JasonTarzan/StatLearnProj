{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests Briefly\n",
    "\n",
    "Random forests fit an ensemble of regression or, in our case, classification trees over the domain of an outcome - here the attitude towards climate change associated with a tweet. \n",
    "\n",
    "Each tree estimator fits a piecewise constant surface over the outcome where the constant is the outcome's grand mean over the included observations. The tree is grown greedily by recursively splitting the observations in feature space at a cut-point in the feature dimension which maximally reduces the misclassification of the attitude outcome. Such trees are grown to maximum depth. \n",
    "\n",
    "In growing an individual tree, we overfit a high variance estimator to the data. Random forests reduce the variance of estimators by averaging over a large number of such trees.\n",
    "\n",
    "A useful upshot of the random selection of features _before_ partitioning observations in feature space is that we are able to assess feature importance. \n",
    "\n",
    "NB The number `n_estimators` of trees averaged is not a real tuning parameter; as with the bootstrap, we need a sufficient number for the estimate to stabilize, but cannot overfit by having too many. In the hyperparameter tuning sections below, I include `n_estimators` in the grid search for convenience, to test if a sufficient number of trees were used. Our lowest cross-validated error is achieved by increasing the number of trees built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Fresh\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports and Set Options\n",
    "\n",
    "import csv  # for slang\n",
    "import os\n",
    "import re  # regex\n",
    "import string  # punct\n",
    "from pprint import pprint\n",
    "\n",
    "import emoji  # for emoji\n",
    "import gensim\n",
    "import keras\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from gensim.models import Word2Vec\n",
    "from IPython.display import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords  # stopwords\n",
    "from nltk.stem import PorterStemmer  # stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import svm, tree\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              RandomForestClassifier, RandomForestRegressor,\n",
    "                              StackingClassifier)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import (accuracy_score, auc, average_precision_score,\n",
    "                             brier_score_loss, classification_report,\n",
    "                             confusion_matrix, f1_score, fbeta_score,\n",
    "                             make_scorer, plot_precision_recall_curve,\n",
    "                             precision_recall_curve, precision_score,\n",
    "                             recall_score, roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, RandomizedSearchCV,\n",
    "                                     cross_val_score, train_test_split)\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC  # \"Support vector classifier\"\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homemade Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Text Class\n",
    "\n",
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def remove_mentions(self, input_text):\n",
    "        '''\n",
    "        Remove mentions, like @Mplamplampla\n",
    "        '''\n",
    "        return re.sub(r'@+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        '''\n",
    "        Remove the urls mention in a tweet\n",
    "        '''\n",
    "        input_text  = ' '.join([w for w in input_text.split(' ') if '.com' not in w])\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        input_text = emoji.demojize(input_text)\n",
    "        input_text = input_text.replace('_','')\n",
    "        input_text = input_text.replace(':','')\n",
    "        return input_text\n",
    "    \n",
    "    def possessive_pronouns(self, input_text):\n",
    "        '''\n",
    "        Remove the possesive pronouns, because otherwise after tokenization we will end up with a word and an s\n",
    "        Example: government's --> [\"government\", \"s\"]\n",
    "        '''\n",
    "        return input_text.replace(\"'s\", \"\")\n",
    "    \n",
    "    def characters(self, input_text):\n",
    "        '''\n",
    "        Remove special and redundant characters that may appear on a tweet and that don't really help in our analysis\n",
    "        '''\n",
    "        input_text = input_text.replace(\"\\r\", \" \") # Carriage Return\n",
    "        input_text = input_text.replace(\"\\n\", \" \") # Newline\n",
    "        input_text = \" \".join(input_text.split()) # Double space\n",
    "        input_text = input_text.replace('\"', '') # Quotes\n",
    "        return input_text\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        '''\n",
    "        Remove punctuation and specifically these symbols '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        '''\n",
    "        punct = string.punctuation # string with all the punctuation symbols '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    \n",
    "    def remove_digits(self, input_text):\n",
    "        '''\n",
    "        Remove numbers\n",
    "        '''\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        '''\n",
    "        Convert all the sentences(words) to lowercase\n",
    "        '''\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        '''\n",
    "        Remove stopwords (refers to the most common words in a language)\n",
    "        '''\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def stemming(self, input_text):\n",
    "        '''\n",
    "        Reduce the words to their stem\n",
    "        '''\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def encode_decode(self, input_text):\n",
    "        '''\n",
    "        Remove weird characters that are result of encoding problems\n",
    "        '''\n",
    "        return  \" \".join([k.encode(\"ascii\", \"ignore\").decode() for k in input_text.split(\" \")])\n",
    "    \n",
    "    \n",
    "    def translator(self, input_text):\n",
    "        '''\n",
    "        Transform abbrevations to normal words\n",
    "        Example: asap --> as soon as possible\n",
    "        '''\n",
    "        input_text = input_text.split(\" \")\n",
    "        j = 0\n",
    "        for _str in input_text:\n",
    "            # File path which consists of Abbreviations.\n",
    "            fileName = r\"slang.txt\"\n",
    "            # File Access mode [Read Mode]\n",
    "            accessMode = \"r\"\n",
    "            with open(fileName, accessMode) as myCSVfile:\n",
    "                # Reading file as CSV with delimiter as \"=\", so that \n",
    "                # abbreviation are stored in row[0] and phrases in row[1]\n",
    "                dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "                # Removing Special Characters.\n",
    "                _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "                for row in dataFromFile:\n",
    "                    # Check if selected word matches short forms[LHS] in text file.\n",
    "                    if _str.upper() == row[0]:\n",
    "                        # If match found replace it with its appropriate phrase in text file.\n",
    "                        input_text[j] = row[1]\n",
    "                myCSVfile.close()\n",
    "            j = j + 1\n",
    "        \n",
    "        return(' '.join(input_text))\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = (X.apply(self.translator)\n",
    "                    .apply(self.remove_mentions)\n",
    "                    .apply(self.remove_urls)\n",
    "                    .apply(self.emoji_oneword)\n",
    "                    .apply(self.possessive_pronouns)\n",
    "                    .apply(self.remove_punctuation)\n",
    "                    .apply(self.remove_digits)\n",
    "                    .apply(self.encode_decode)\n",
    "                    .apply(self.characters)\n",
    "                    .apply(self.to_lower)\n",
    "                    .apply(self.remove_stopwords)\n",
    "                    .apply(self.stemming))\n",
    "        return clean_X\n",
    "    \n",
    "    def transform_no_stem(self, X, **transform_params):\n",
    "        clean_X = (X.apply(self.translator)\n",
    "                    .apply(self.remove_mentions)\n",
    "                    .apply(self.remove_urls)\n",
    "                    .apply(self.emoji_oneword)\n",
    "                    .apply(self.possessive_pronouns)\n",
    "                    .apply(self.remove_punctuation)\n",
    "                    .apply(self.remove_digits)\n",
    "                    .apply(self.encode_decode)\n",
    "                    .apply(self.characters)\n",
    "                    .apply(self.to_lower)\n",
    "                    .apply(self.remove_stopwords))\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data and Create Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WWF Climate &amp; Energy</td>\n",
       "      <td>climateWWF</td>\n",
       "      <td>2020-04-28</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>Economic recovery and national climate pledges...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WWF Climate &amp; Energy</td>\n",
       "      <td>climateWWF</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>In this difficult time, it’s hard to connect w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WWF Climate &amp; Energy</td>\n",
       "      <td>climateWWF</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>43</td>\n",
       "      <td>69</td>\n",
       "      <td>The decision to postpone # COP26, is unavoidab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WWF Climate &amp; Energy</td>\n",
       "      <td>climateWWF</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>Japan - the world’s fifth largest emitter of g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WWF Climate &amp; Energy</td>\n",
       "      <td>climateWWF</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>22</td>\n",
       "      <td>40</td>\n",
       "      <td>How can countries include # NatureBasedSolutio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               username user_handle        date retweets favorites                                               text  label\n",
       "0  WWF Climate & Energy  climateWWF  2020-04-28       11        22  Economic recovery and national climate pledges...      0\n",
       "1  WWF Climate & Energy  climateWWF  2020-04-22        6        16  In this difficult time, it’s hard to connect w...      0\n",
       "2  WWF Climate & Energy  climateWWF  2020-04-01       43        69  The decision to postpone # COP26, is unavoidab...      0\n",
       "3  WWF Climate & Energy  climateWWF  2020-03-30       24        30  Japan - the world’s fifth largest emitter of g...      0\n",
       "4  WWF Climate & Energy  climateWWF  2020-03-30       22        40  How can countries include # NatureBasedSolutio...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data (Raw copy for reference; copy for processing)\n",
    "\n",
    "tweets_raw = pd.read_csv('https://github.com/anilkeshwani/StatLearnProj/raw/master/Iason/climate_change_tweets_sample-2020-05-16-17-57.csv')\n",
    "tweets = pd.read_csv('https://github.com/anilkeshwani/StatLearnProj/raw/master/Iason/climate_change_tweets_sample-2020-05-16-17-57.csv')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Dataset\n",
    "\n",
    "Applies the class methods (leveraging `sklearn` API):\n",
    "\n",
    "- translator\n",
    "- remove_mentions\n",
    "- remove_urls\n",
    "- emoji_oneword\n",
    "- possessive_pronouns\n",
    "- remove_punctuation\n",
    "- remove_digits\n",
    "- encode_decode\n",
    "- characters\n",
    "- to_lower\n",
    "- remove_stopwords\n",
    "- stemming (via Porter Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "\n",
    "# ct = CleanText()\n",
    "# tweets[\"text\"] = ct.fit_transform(tweets.text)\n",
    "# tweets.to_csv(\"clean_tweets.csv\") # save once processed\n",
    "tweets = pd.read_csv(\"clean_tweets.csv\") # read in instead\n",
    "tweets = tweets.loc[(~tweets.text.isnull()), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3642     might progress climat area not enough global s...\n",
      "12695    trump crackdown politic scienc nasa climat div...\n",
      "8451     no one would believ human panick fals climat m...\n",
      "Name: text, dtype: object (14406,)\n",
      "\n",
      "8376     nazi root environment climat chang fraud bbcne...\n",
      "6111     interest democrat candid compar mani aspect cl...\n",
      "13983    ittrademark imposs see global warm signal minn...\n",
      "Name: text, dtype: object (3602,)\n",
      "\n",
      "3642     0\n",
      "12695    1\n",
      "8451     1\n",
      "Name: label, dtype: int64 (14406,)\n",
      "\n",
      "8376     1\n",
      "6111     0\n",
      "13983    1\n",
      "Name: label, dtype: int64 (3602,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(tweets.text, tweets.label, \n",
    "                                                    test_size=0.2, random_state=17, \n",
    "                                                    shuffle=True) # explicit default\n",
    "\n",
    "[print(dat.head(3), dat.shape, end=\"\\n\"*2) for dat in [X_train, X_test, Y_train, Y_test]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label counts: \n",
      "1    8433\n",
      "0    5973\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Test label counts: \n",
      "1    2138\n",
      "0    1464\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training label counts: \\n{Y_train.value_counts()}\", end=\"\\n\"*2)\n",
    "print(f\"Test label counts: \\n{Y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save set of workspace objects' names to enable periodic clean-up\n",
    "\n",
    "necessities = set(dir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CLIMATE CHANGE: Obama's right, in a way: if we change the political climate & get rid of progressives, we can help our national security!\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_raw.iloc[14156][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BOW) Binary (\"One-Hot\") Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words Representation (One Hot, i.e. binary)\n",
    "\n",
    "BOW_vectorizer = CountVectorizer(stop_words = 'english', \n",
    "                                 binary=True, # Creates 0/1 \"One Hot\" vector; \n",
    "                                              # np.unique(BOW_train.toarray())\n",
    "                                 min_df = 10)\n",
    "BOW_vectorizer.fit(X_train)\n",
    "BOW_train = BOW_vectorizer.transform(X_train)\n",
    "BOW_test = BOW_vectorizer.transform(X_test)\n",
    "\n",
    "# Construct Scaled Datasets\n",
    "\n",
    "scaler_BOW = MaxAbsScaler()\n",
    "BOW_train_scaled = scaler_BOW.fit_transform(BOW_train)\n",
    "BOW_test_scaled = scaler_BOW.transform(BOW_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zero', 2206),\n",
       " ('yr', 2205),\n",
       " ('youtub', 2204),\n",
       " ('youthvgov', 2203),\n",
       " ('youthtopow', 2202),\n",
       " ('youthclimatesummit', 2201),\n",
       " ('youth', 2200),\n",
       " ('young', 2199),\n",
       " ('york', 2198),\n",
       " ('yesterday', 2197),\n",
       " ('year', 2196),\n",
       " ('yeah', 2195),\n",
       " ('ye', 2194),\n",
       " ('yall', 2193),\n",
       " ('yale', 2192),\n",
       " ('ya', 2191),\n",
       " ('wwf', 2190),\n",
       " ('wsj', 2189),\n",
       " ('wrote', 2188),\n",
       " ('wrong', 2187)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most frequently occurring words in the training corpus\n",
    "\n",
    "[(index, word) for index, word in sorted(BOW_vectorizer.vocabulary_.items(), key=lambda item: item[1], reverse=True)][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words with Frequencies Representation (FBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words Representation (Frequencies; binary=False)\n",
    "\n",
    "FBOW_vectorizer = CountVectorizer(stop_words = 'english', \n",
    "                                  binary=False, # Creates Word Frequency Vector; \n",
    "                                                # # np.unique(FBOW_train.toarray())\n",
    "                                  min_df = 10)\n",
    "FBOW_vectorizer.fit(X_train)\n",
    "FBOW_train = FBOW_vectorizer.transform(X_train)\n",
    "FBOW_test = FBOW_vectorizer.transform(X_test)\n",
    "\n",
    "# Construct Scaled Datasets\n",
    "\n",
    "scaler_FBOW = MaxAbsScaler()\n",
    "FBOW_train_scaled = scaler_FBOW.fit_transform(FBOW_train)\n",
    "FBOW_test_scaled = scaler_FBOW.transform(FBOW_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 13], dtype=int64), array([31661370,   127070,     5161,      379,       45,        7,\n",
      "              5,        1,        2,        1,        1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Word use (per tweet) frequencies\n",
    "\n",
    "print(np.unique(FBOW_train.toarray(), return_counts=True))\n",
    "\n",
    "# Feature_Index: Word Mapping\n",
    "\n",
    "# {v: k for k, v in FBOW_vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Bigrams (bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(stop_words = 'english', \n",
    "                                    binary=True, \n",
    "                                    min_df = 10,\n",
    "                                    ngram_range = (1,2)) # create bigrams\n",
    "bigram_vectorizer.fit(X_train)\n",
    "\n",
    "bigram_train = bigram_vectorizer.transform(X_train)\n",
    "bigram_test = bigram_vectorizer.transform(X_test)\n",
    "\n",
    "# Construct Scaled Datasets\n",
    "\n",
    "scaler_bigram = MaxAbsScaler()\n",
    "bigram_train_scaled = scaler_bigram.fit_transform(bigram_train)\n",
    "bigram_test_scaled = scaler_bigram.transform(bigram_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency–Inverse Document Frequency Representation (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                   min_df=10) # used for now for consistency\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "tfidf_train = tfidf_vectorizer.transform(X_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Construct Scaled Datasets\n",
    "\n",
    "scaler_tfidf = MaxAbsScaler()\n",
    "tfidf_train_scaled = scaler_tfidf.fit_transform(tfidf_train)\n",
    "tfidf_test_scaled = scaler_tfidf.transform(tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder: Available objects produced by `fit`\n",
    "\n",
    "Manual inspection of the attributes added after `fit`ting is useful; these are identified with a trailing underscore, e.g. `Cs_` or `scores_`\n",
    "\n",
    "The `score_` attribute is a dictionary with one entry per outcome class; in our binary case, only the class `1` is stored in the dictionary. \n",
    "\n",
    "Each entry has dimension `(n_folds, n_cs, n_l1_ratios)`; i.e.  `(n_arrays, n_rows, n_cols)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objects produced by fit\n",
    "\n",
    "# print(\"Fit Objects:\")\n",
    "\n",
    "# for item in dir(sklearn_Estimator):\n",
    "#     if item.endswith(\"_\") and (not item.startswith(\"_\")):\n",
    "#         print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests: BOW Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=True, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(oob_score=True)\n",
    "\n",
    "rf_clf.fit(BOW_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 0.9991670137442732\n",
      "Out-of-Bag Score: 0.8892822435096488\n",
      "Cross-validated accuracy : [0.89417071 0.88441513 0.88719195 0.88996876 0.88024991]\n",
      "Mean CV accuracy : 0.887\n",
      "Test set score : 0.8903387007218212\n"
     ]
    }
   ],
   "source": [
    "rf_clf_cv_score = cross_val_score(rf_clf, BOW_train, Y_train)\n",
    "\n",
    "print(f\"Training Set Accuracy: {rf_clf.score(BOW_train, Y_train)}\")\n",
    "print(f\"Out-of-Bag Score: {rf_clf.oob_score_}\")\n",
    "\n",
    "print(f\"Cross-validated accuracy : {rf_clf_cv_score}\") \n",
    "print(f\"Mean CV accuracy : {np.round(np.mean(rf_clf_cv_score), 3)}\")\n",
    "\n",
    "print(f\"Test set score : {rf_clf.score(BOW_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=True, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_scaled = RandomForestClassifier(oob_score=True)\n",
    "\n",
    "rf_clf_scaled.fit(BOW_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 0.9991670137442732\n",
      "Out-of-Bag Score: 0.8890045814244065\n",
      "Cross-validated accuracy : [0.888966   0.88337383 0.88267963 0.88753905 0.88406803]\n",
      "Mean CV accuracy : 0.885\n",
      "Test set score : 0.8900610771793448\n"
     ]
    }
   ],
   "source": [
    "rf_clf_scaled_cv_score = cross_val_score(rf_clf_scaled, BOW_train_scaled, Y_train)\n",
    "\n",
    "print(f\"Training Set Accuracy: {rf_clf_scaled.score(BOW_train_scaled, Y_train)}\")\n",
    "print(f\"Out-of-Bag Score: {rf_clf_scaled.oob_score_}\")\n",
    "\n",
    "print(f\"Cross-validated accuracy : {rf_clf_scaled_cv_score}\") \n",
    "print(f\"Mean CV accuracy : {np.round(np.mean(rf_clf_scaled_cv_score), 3)}\")\n",
    "\n",
    "print(f\"Test set score : {rf_clf_scaled.score(BOW_test_scaled, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter Tuning via Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB The below cell only appears once. The same hyperparameter grid and CV process is used across for all (subsequent) tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Hyperparameter Search Space\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 100, None],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 3, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [50, 100, 300]\n",
    "}\n",
    "\n",
    "# Set Cross-Validation Process\n",
    "\n",
    "kfcv = KFold(n_splits=5, shuffle=True, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "rf_grid_search = GridSearchCV(estimator = rf_clf, param_grid = param_grid, \n",
    "                              cv = kfcv, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed: 10.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=101, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_f...\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=True, random_state=None,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [80, 100, None],\n",
       "                         'max_features': ['sqrt', 'log2'],\n",
       "                         'min_samples_leaf': [1, 3, 5],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [50, 100, 300]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid_search.fit(BOW_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9021933166181745\n",
      "\n",
      "Best parameters:\n",
      "bootstrap: True\n",
      "max_depth: None\n",
      "max_features: log2\n",
      "min_samples_leaf: 1\n",
      "min_samples_split: 10\n",
      "n_estimators: 300\n"
     ]
    }
   ],
   "source": [
    "# Best Score\n",
    "\n",
    "print(f\"Best Score: {rf_grid_search.best_score_}\", end=\"\\n\"*2)\n",
    "\n",
    "# Best Parameters\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "for k, v in rf_grid_search.best_params_.items():\n",
    "    print(str(k) + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests: Bigrams Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=True, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_bigram = RandomForestClassifier(oob_score=True)\n",
    "\n",
    "rf_clf_bigram.fit(bigram_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 0.9992364292655838\n",
      "Out-of-Bag Score: 0.8939330834374566\n",
      "Cross-validated accuracy : [0.89486468 0.88719195 0.89066296 0.89239847 0.88649774]\n",
      "Mean CV accuracy : 0.89\n",
      "Test set score : 0.8911715713492504\n"
     ]
    }
   ],
   "source": [
    "rf_clf_bigram_cv_score = cross_val_score(rf_clf_bigram, bigram_train, Y_train)\n",
    "\n",
    "print(f\"Training Set Accuracy: {rf_clf_bigram.score(bigram_train, Y_train)}\")\n",
    "print(f\"Out-of-Bag Score: {rf_clf_bigram.oob_score_}\")\n",
    "\n",
    "print(f\"Cross-validated accuracy : {rf_clf_bigram_cv_score}\") \n",
    "print(f\"Mean CV accuracy : {np.round(np.mean(rf_clf_bigram_cv_score), 3)}\")\n",
    "\n",
    "print(f\"Test set score : {rf_clf_bigram.score(bigram_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter Tuning via Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "rf_grid_search_bigram = GridSearchCV(estimator = rf_clf_bigram, param_grid = param_grid, \n",
    "                                     cv = kfcv, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   28.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed: 10.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=101, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_f...\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=True, random_state=None,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [80, 100, None],\n",
       "                         'max_features': ['sqrt', 'log2'],\n",
       "                         'min_samples_leaf': [1, 3, 5],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [50, 100, 300]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid_search_bigram.fit(bigram_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9061502278321608\n",
      "\n",
      "Best parameters:\n",
      "bootstrap: True\n",
      "max_depth: None\n",
      "max_features: log2\n",
      "min_samples_leaf: 1\n",
      "min_samples_split: 10\n",
      "n_estimators: 300\n"
     ]
    }
   ],
   "source": [
    "# Best Score\n",
    "\n",
    "print(f\"Best Score: {rf_grid_search_bigram.best_score_}\", end=\"\\n\"*2)\n",
    "\n",
    "# Best Parameters\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "for k, v in rf_grid_search_bigram.best_params_.items():\n",
    "    print(str(k) + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests: FBOW Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=True, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_FBOW = RandomForestClassifier(oob_score=True)\n",
    "\n",
    "rf_clf_FBOW.fit(FBOW_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 0.9992364292655838\n",
      "Out-of-Bag Score: 0.8867138692211578\n",
      "Cross-validated accuracy : [0.89243581 0.88441513 0.88510934 0.88684485 0.88129122]\n",
      "Mean CV accuracy : 0.886\n",
      "Test set score : 0.88811771238201\n"
     ]
    }
   ],
   "source": [
    "rf_clf_FBOW_cv_score = cross_val_score(rf_clf_FBOW, FBOW_train, Y_train)\n",
    "\n",
    "print(f\"Training Set Accuracy: {rf_clf_FBOW.score(FBOW_train, Y_train)}\")\n",
    "print(f\"Out-of-Bag Score: {rf_clf_FBOW.oob_score_}\")\n",
    "\n",
    "print(f\"Cross-validated accuracy : {rf_clf_FBOW_cv_score}\") \n",
    "print(f\"Mean CV accuracy : {np.round(np.mean(rf_clf_FBOW_cv_score), 3)}\")\n",
    "\n",
    "print(f\"Test set score : {rf_clf_FBOW.score(FBOW_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=True, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_FBOW_scaled = RandomForestClassifier(oob_score=True)\n",
    "\n",
    "rf_clf_FBOW_scaled.fit(FBOW_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 0.9992364292655838\n",
      "Out-of-Bag Score: 0.8890045814244065\n",
      "Cross-validated accuracy : [0.89174185 0.88684485 0.88545644 0.88858035 0.88337383]\n",
      "Mean CV accuracy : 0.887\n",
      "Test set score : 0.889505830094392\n"
     ]
    }
   ],
   "source": [
    "rf_clf_FBOW_scaled_cv_score = cross_val_score(rf_clf_FBOW_scaled, FBOW_train_scaled, Y_train)\n",
    "\n",
    "print(f\"Training Set Accuracy: {rf_clf_FBOW_scaled.score(FBOW_train_scaled, Y_train)}\")\n",
    "print(f\"Out-of-Bag Score: {rf_clf_FBOW_scaled.oob_score_}\")\n",
    "\n",
    "print(f\"Cross-validated accuracy : {rf_clf_FBOW_scaled_cv_score}\") \n",
    "print(f\"Mean CV accuracy : {np.round(np.mean(rf_clf_FBOW_scaled_cv_score), 3)}\")\n",
    "\n",
    "print(f\"Test set score : {rf_clf_FBOW_scaled.score(FBOW_test_scaled, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter Tuning via Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "rf_grid_search_FBOW = GridSearchCV(estimator = rf_clf_FBOW, param_grid = param_grid, \n",
    "                                   cv = kfcv, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed:  9.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=101, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_f...\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=True, random_state=None,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [80, 100, None],\n",
       "                         'max_features': ['sqrt', 'log2'],\n",
       "                         'min_samples_leaf': [1, 3, 5],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [50, 100, 300]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid_search_FBOW.fit(FBOW_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9033042347611875\n",
      "\n",
      "Best parameters:\n",
      "bootstrap: True\n",
      "max_depth: None\n",
      "max_features: log2\n",
      "min_samples_leaf: 1\n",
      "min_samples_split: 10\n",
      "n_estimators: 300\n"
     ]
    }
   ],
   "source": [
    "# Best Score\n",
    "\n",
    "print(f\"Best Score: {rf_grid_search_FBOW.best_score_}\", end=\"\\n\"*2)\n",
    "\n",
    "# Best Parameters\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "for k, v in rf_grid_search_FBOW.best_params_.items():\n",
    "    print(str(k) + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning for Scaled Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "rf_grid_search_FBOW_scaled = GridSearchCV(estimator = rf_clf_FBOW_scaled, param_grid = param_grid, \n",
    "                                          cv = kfcv, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   26.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed:  9.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=101, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_f...\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=True, random_state=None,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [80, 100, None],\n",
       "                         'max_features': ['sqrt', 'log2'],\n",
       "                         'min_samples_leaf': [1, 3, 5],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [50, 100, 300]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid_search_FBOW_scaled.fit(FBOW_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9024709498036986\n",
      "\n",
      "Best parameters:\n",
      "bootstrap: True\n",
      "max_depth: None\n",
      "max_features: log2\n",
      "min_samples_leaf: 1\n",
      "min_samples_split: 10\n",
      "n_estimators: 300\n"
     ]
    }
   ],
   "source": [
    "# Best Score\n",
    "\n",
    "print(f\"Best Score: {rf_grid_search_FBOW_scaled.best_score_}\", end=\"\\n\"*2)\n",
    "\n",
    "# Best Parameters\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "for k, v in rf_grid_search_FBOW_scaled.best_params_.items():\n",
    "    print(str(k) + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests: TF-IDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=True, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_tfidf = RandomForestClassifier(oob_score=True)\n",
    "\n",
    "rf_clf_tfidf.fit(tfidf_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 0.9992364292655838\n",
      "Out-of-Bag Score: 0.8848396501457726\n",
      "Cross-validated accuracy : [0.88619015 0.87643179 0.88684485 0.88649774 0.88372093]\n",
      "Mean CV accuracy : 0.884\n",
      "Test set score : 0.8847862298722932\n"
     ]
    }
   ],
   "source": [
    "rf_clf_tfidf_cv_score = cross_val_score(rf_clf_tfidf, tfidf_train, Y_train)\n",
    "\n",
    "print(f\"Training Set Accuracy: {rf_clf_tfidf.score(tfidf_train, Y_train)}\")\n",
    "print(f\"Out-of-Bag Score: {rf_clf_tfidf.oob_score_}\")\n",
    "\n",
    "print(f\"Cross-validated accuracy : {rf_clf_tfidf_cv_score}\") \n",
    "print(f\"Mean CV accuracy : {np.round(np.mean(rf_clf_tfidf_cv_score), 3)}\")\n",
    "\n",
    "print(f\"Test set score : {rf_clf_tfidf.score(tfidf_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=True, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_tfidf_scaled = RandomForestClassifier(oob_score=True)\n",
    "\n",
    "rf_clf_tfidf_scaled.fit(tfidf_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 0.9992364292655838\n",
      "Out-of-Bag Score: 0.8850478967097043\n",
      "Cross-validated accuracy : [0.89000694 0.88129122 0.88337383 0.88441513 0.8781673 ]\n",
      "Mean CV accuracy : 0.883\n",
      "Test set score : 0.8831204886174348\n"
     ]
    }
   ],
   "source": [
    "rf_clf_tfidf_scaled_cv_score = cross_val_score(rf_clf_tfidf_scaled, tfidf_train_scaled, Y_train)\n",
    "\n",
    "print(f\"Training Set Accuracy: {rf_clf_tfidf_scaled.score(tfidf_train_scaled, Y_train)}\")\n",
    "print(f\"Out-of-Bag Score: {rf_clf_tfidf_scaled.oob_score_}\")\n",
    "\n",
    "print(f\"Cross-validated accuracy : {rf_clf_tfidf_scaled_cv_score}\") \n",
    "print(f\"Mean CV accuracy : {np.round(np.mean(rf_clf_tfidf_scaled_cv_score), 3)}\")\n",
    "\n",
    "print(f\"Test set score : {rf_clf_tfidf_scaled.score(tfidf_test_scaled, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter Tuning via Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "rf_grid_search_tfidf = GridSearchCV(estimator = rf_clf_tfidf, param_grid = param_grid, \n",
    "                                    cv = kfcv, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   27.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed: 10.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=101, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_f...\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=True, random_state=None,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [80, 100, None],\n",
       "                         'max_features': ['sqrt', 'log2'],\n",
       "                         'min_samples_leaf': [1, 3, 5],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [50, 100, 300]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid_search_tfidf.fit(tfidf_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.898791984913481\n",
      "\n",
      "Best parameters:\n",
      "bootstrap: True\n",
      "max_depth: None\n",
      "max_features: log2\n",
      "min_samples_leaf: 1\n",
      "min_samples_split: 2\n",
      "n_estimators: 300\n"
     ]
    }
   ],
   "source": [
    "# Best Score\n",
    "\n",
    "print(f\"Best Score: {rf_grid_search_tfidf.best_score_}\", end=\"\\n\"*2)\n",
    "\n",
    "# Best Parameters\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "for k, v in rf_grid_search_tfidf.best_params_.items():\n",
    "    print(str(k) + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning for Scaled Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "rf_grid_search_tfidf_scaled = GridSearchCV(estimator = rf_clf_tfidf_scaled, param_grid = param_grid, \n",
    "                                           cv = kfcv, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   28.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed: 10.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=101, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_f...\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=True, random_state=None,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [80, 100, None],\n",
       "                         'max_features': ['sqrt', 'log2'],\n",
       "                         'min_samples_leaf': [1, 3, 5],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [50, 100, 300]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid_search_tfidf_scaled.fit(tfidf_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9003887972624973\n",
      "\n",
      "Best parameters:\n",
      "bootstrap: True\n",
      "max_depth: None\n",
      "max_features: log2\n",
      "min_samples_leaf: 1\n",
      "min_samples_split: 2\n",
      "n_estimators: 300\n"
     ]
    }
   ],
   "source": [
    "# Best Score\n",
    "\n",
    "print(f\"Best Score: {rf_grid_search_tfidf_scaled.best_score_}\", end=\"\\n\"*2)\n",
    "\n",
    "# Best Parameters\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "for k, v in rf_grid_search_tfidf_scaled.best_params_.items():\n",
    "    print(str(k) + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18009"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tweets_raw.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3201 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_train[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
