# Journal Club Presentation

- [Presentation on Google Slides](https://docs.google.com/presentation/d/1C8SewYulr1CkB7qcuimrjc6La-gb_5yinER-bR66K1k/edit#slide=id.g7705cb3155_18_24)
- [Iason's code replicating the paper's results (Github repository)](https://github.com/JasonTarzan/Deep-Double-Descent-Where-Bigger-Models-and-More-Data-Hurts/blob/master/Deep-Double-Descent.ipynb)

### Literature and Resources

- **[Nakkiran et al. (2019) Deep Double Descent: Where Bigger Models and More Data Hurt](https://arxiv.org/abs/1912.02292)**; link to [PDF](https://arxiv.org/pdf/1912.02292)
- [Write-Up of Paper by authors Nakkiran et al. on OpenAI](https://openai.com/blog/deep-double-descent/)
- [_Understanding “Deep Double Descent”_](https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent) - Post at _LESSWRONG_ reviewing double deep descent phenomenon including background and the paper we're interested in
- [Deep Double Descent: when more data is a bad thing](https://towardsdatascience.com/deep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538)

- **[Belkin et al. (2019) Reconciling modern machine learning practice and the bias-variance trade-of](https://arxiv.org/pdf/1812.11118.pdf)**; link to [PDF](https://arxiv.org/pdf/1812.11118.pdf):

> ...we reconcile the classical understanding and the modern practice within a unified performance curve. This "double descent" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.

- [Madhu S. Advani and Andrew M. Saxe (2017) _High-dimensional dynamics of generalization error in neural networks_](https://arxiv.org/abs/1710.03667):  

> Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that naive application of worst-case theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.

- [Tomaso Poggio, Qianli Liao & Andrzej Banburski (2020) Complexity control by gradient descent in deep networks](https://www.nature.com/articles/s41467-020-14663-9)

- [DeepAI: What is an Epoch?](https://deepai.org/machine-learning-glossary-and-terms/epoch)
- [Epoch vs Batch Size vs Iterations](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9) - Medium post by Sagar Sharma
- [An Overview of ResNet and its Variants](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035) - Medium post by Vincent Fung

- [playground.tensorflow.org - Visualisation of Neural Network](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.38437&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)
- [Residual Neural Network ("ResNet")](https://en.wikipedia.org/wiki/Residual_neural_network)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Post from Jay Alammar (detailed)
- [How Transformers Work](https://towardsdatascience.com/transformers-141e32e69591) - Medium Post by Giuliano Giacaglia
- [What is a Transformer](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04) - Medium post by Maxime Allard

- [CIFAR-10 and CIFAR-100 datasets](https://www.cs.toronto.edu/~kriz/cifar.html) - _The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton_



