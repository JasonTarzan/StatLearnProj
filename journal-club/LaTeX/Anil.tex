\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Paper Write-up: Nakkiran et al. (2019) Deep Double Descent},
            pdfauthor={Anil Keshwani},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Paper Write-up: Nakkiran et al. (2019) Deep Double Descent}
\author{Anil Keshwani}
\date{2020-05-10 00:00:00 +0100}

\begin{document}
\maketitle

\begin{quote}
This post is a write-up of
\href{https://arxiv.org/abs/1912.02292}{Nakkiran et al. (2019) Deep
Double Descent: Where Bigger Models And More Data Hurt} which was
presented at a journal club as part of our course in Statistical
Learning with Pierpaolo Brutti
\end{quote}

\hypertarget{one-line-summary}{%
\paragraph{One line summary}\label{one-line-summary}}

The authors demonstrate that deep learning tasks exhibit
``double-descent'' wherein model performance gets worse \emph{then
better} as a function of model size and generalise this phenomenon to
training epochs via their \emph{effective model complexity} metric
showing that larger samples sizes can diminsh model performance in some
conditions.

\hypertarget{introduction}{%
\paragraph{Introduction}\label{introduction}}

\begin{itemize}
\tightlist
\item
  Modern neural networks have millions of parameters and yet outperform
  more parsimonious models in contrast to the notion that \emph{larger
  models are worse} stemming from the bias-variance trade-off of
  classical statistics\footnote{The distinction between classical
    statistics and modern regimes is one asserted by the authors.};
  i.e.~models with large numbers of parameters ought to perform worse as
  they fit noise (i.e.~sample variance).
\item
  The best strategy with respect to training time is also questioned:
  should optimal early stopping - wherein models are trained until an
  increase in the empirically estimated test error is observed - be
  employed or do regularised models with zero-training error have better
  performance?
\end{itemize}

The authors distinguish \emph{under-parametrised} models from those
which are able to \emph{interpolate} data (i.e.~achieve approximately
zero training error) in the deep learning setting. They claim that the
former class of models follow the U-shaped curve of test error following
from the bias-variance trade-off but the latter class sees an
improvement (i.e.~decrease) in test error from increased
complexity\footnote{The authors point out that similar behavior was
  previously observed in Manfred Opper (1995) \emph{Statistical
  mechanics of learning: Generalization. The Handbook of Brain Theory
  and Neural Networks}, pp.~922-925; Manfred Opper (2001) \emph{Learning
  to generalize. Frontiers of Life}, 3 part 2, pp.~763-775; Advani and
  Saxe (2017) High-dimensional dynamics of generalization error in
  neural networks. arXiv preprint arXiv:1710.03667; Spigler et al.
  (2018) A jamming transition from under-to over-parametrization affects
  loss landscape and generalization arXiv preprint arXiv:1810.09665; and
  Geiger et a. (2019) Jamming transition as a paradigm to understand the
  loss landscape of deep neural networks. Physical Review E,
  100(1):012115. They note most significantly that the phenomenon was
  first postulated in generality by Belkin et al. (2018) who named it
  \emph{double descent} and demonstrated it for decision trees, random
  features, and 2-layer neural networks using an \(l_2\) loss on a
  variety of learning tasks including MNIST and CIFAR-10.}.

\hypertarget{main-contributions}{%
\paragraph{Main Contributions}\label{main-contributions}}

The main novel contributions of the authors in this paper were as
follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Variously demonstrate double descent in
  \href{https://en.wikipedia.org/wiki/Convolutional_neural_network}{CNNs},
  \href{https://en.wikipedia.org/wiki/Residual_neural_network}{ResNets}
  and
  \href{https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)}{Transformers}\footnote{See
    also
    \href{https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html}{this
    Google AI Blog post} on transformers.} using the
  \href{https://en.wikipedia.org/wiki/CIFAR-10}{CIFAR 10},
  \href{https://www.cs.toronto.edu/~kriz/cifar.html}{CIFAR 100},
  \href{http://www.statmt.org/wmt14/translation-task.html}{IWSLT '14
  de-en} or
  \href{https://www.tensorflow.org/datasets/catalog/wmt14_translate}{WMT
  '14 en-fr} datasets (see Table A from the paper and shown below).
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Define the \emph{effective model complexity} (EMC) metric: the maximum
  number of samples on which a training procedure can achieve close to
  zero training error. The EMC depends on the model complexity but also
  on the training time (measured in epochs) the procedure is allowed.\\
\item
  Demonstrate epoch-wise double descent for models with sufficient
  complexity (i.e.~parameters) to interpolate the data.\\
\item
  Demonstrate decrease in model performance (increase in test error)
  with increasing sample size for models lying in a critical interval of
  model complexity: ``more data hurts''.
\item
  The authors also test the effects of label noise\footnote{Corruption
    of the training sample labels. Here done experimentally by
    mislabelling the outcome label (the \(y_i\) elements of the 2-tuple
    in the set, \(S\)) with some probability, \(p\), i.e.~according to a
    Bernoulli distribution.} remarking that double descent was observed
  ``most strongly in settings with label noise'' and comment that label
  noise constitutes a proxy for model mis-specification, i.e.~inaccurate
  modelling assumptions.
\end{enumerate}

\hypertarget{formalised-hypothesis}{%
\paragraph{Formalised Hypothesis}\label{formalised-hypothesis}}

Defining a \emph{training procedure}, \(\mathcal{T}\) as any procedure
taking as input a set \(S = \{(X_1, y_1), \dots, (x_n, y_n)\}\) of
labelled training examples and giving as output a classifier
\(\mathcal{T}(S)\) mapping data to labels, the authors define
\emph{effective model complexity} of \(\mathcal{T}\) w.r.t. distribution
\(\mathcal{D}\) and with parameter
\(\epsilon \geq 0\){[}\^{}choose-epsilon{]} to be the maximum number of
samples, \(n\), on which \(\mathcal{T}\) achieves on average
\(\approx 0\) training error:

\[EMC_{\mathcal{D}, \epsilon}(\mathcal{T}) \triangleq max\{n\ \vert\ \mathbb{E}_{S \sim D^n}[Error_S(\mathcal{T}(S))] \leq \epsilon \}\]

where \(Error_S(M)\) is the \emph{mean error} of model \(M\) on training
samples \(S\).

They thus propose the following hypotheses:\\
- \textbf{Under-parametrised regimes}: If
\(EMC_{\mathcal{D}, \epsilon}(\mathcal{T})\) is \emph{sufficiently
smaller} than the sample size \(n\), perturbations to \(\mathcal{T}\)
that increase its complexity will \textbf{decrease} its test error.\\
- \textbf{Over-parametrised regimes}: If
\(EMC_{\mathcal{D}, \epsilon}(\mathcal{T})\) is \emph{sufficiently
larger} than the sample size \(n\), perturbations to \(\mathcal{T}\)
that increase its complexity will \textbf{decrease} its test error.\\
- \textbf{Critically parametrised regimes}: If
\(EMC_{\mathcal{D}, \epsilon}(\mathcal{T}) \approx n\), perturbations to
\(\mathcal{T}\) that increase its complexity may \textbf{increase or
decrease} its test error.

\hypertarget{experimental-methods}{%
\paragraph{Experimental Methods}\label{experimental-methods}}

The authors use:\\
- \textbf{ResNets} - ResNet18s are parametrised by varying the width
hyperparameter of convolutional layer, i.e.~number of filters:
specifically layer widths \((k, 2k, 4k, 8k)\) for various \(k\); default
\(k=64\).\\
- \textbf{Standard Convolutional Neural Networks (CNNs)} - 5-layer CNNs
with 4 convolutional layers of widths \((k, 2k, 4k, 8k)\) for various
\(k\) and a fully-connected layer\footnote{The authors note: "the CNN
  with width \(k = 64\), can reach over 90\% test accuracy on CIFAR-10
  with data augmentation}.\\
- \textbf{Transformers} - 6-layer encoder-decoder scaling the size of
the network by modifying the embedding dimension, \(d_{model}\) and
setting the width of the fully-connected layers according to
\(d_{ff} = 4 \cdot d_{model}\)\\
- \textbf{Optimisation} - ResNets and CNNs were trained with
cross-entropy loss and (1) Adam with learning rate 0.0001 for 4k epochs
and (2) Stochastic Gradient Descent (SGD) with learning rate
\(\propto \frac{1}{\sqrt{T}}\) for 500k gradient steps. Transformers
were trained for 80k gradient steps with 10\% label smoothing and no
drop-out.

\textbf{Label noise} was added experimentally by changing the correct
label in a dataset to a uniformly random incorrect one with probability
\(p\).

\textbf{The main experimental result of the paper is empirical
validation of this hypothesis for different datasets, model
architectures and optimisation methods, as well as different
interpolation thresholds gained by varying the number of model
parameters, length of training, amount of label noise\footnote{Corruption
  of the training sample labels. Here done experimentally by
  mislabelling the outcome label (the \(y_i\) elements of the 2-tuple in
  the set, \(S\)) with some probability, \(p\), i.e.~according to a
  Bernoulli distribution.} in the distribution and the number of
training samples.}

\hypertarget{critique-and-perspective}{%
\paragraph{Critique and Perspective}\label{critique-and-perspective}}

\begin{itemize}
\tightlist
\item
  As mentioned in Belkin et al. (2018), some optimisation methods such
  as stochastic gradient descent may be sensitive to the starting
  parameter values and therefore may not converge to optima with low
  test set loss, thereby masking the double descent phenomenon.
\item
  Nakkiran and colleagues note themselves that they do not have a
  principled way of assigning the value of \(\epsilon\) or defining when
  \(EMC_{\mathcal{D}, \epsilon}(\mathcal{T})\) is \emph{sufficiently
  smaller} or \emph{larger} than the sample size \(n\)\footnote{``The
    width of the critical interval {[}around the interpolation threshold
    \(EMC_{\mathcal{D}, \epsilon}(\mathcal{T}) = n\){]} depends on both
    the distribution and the training procedure in ways we do not yet
    completely understand.''}.\\
\item
  The authors demonstrate double descent for a limited number of
  datasets and proof of its generality as a phenomenon would require
  either wider systematic empirical studies on diverse datasets or
  theoretical grounding.
\end{itemize}

\hypertarget{resources}{%
\paragraph{Resources}\label{resources}}

\begin{itemize}
\tightlist
\item
  Raw data from the authors' experiments are available for download
  \href{https://gitlab.com/harvard-machine-learning/double-descent/tree/master}{here}
\end{itemize}

\end{document}
