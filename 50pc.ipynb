{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anilkeshwani/StatLearnProj/blob/master/50pc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnRgXWNywSaV",
        "colab_type": "text"
      },
      "source": [
        "# Master Notebook\n",
        "\n",
        "The aim of this notebook is to create a single strand of analysis with a coherent narrative. \n",
        "\n",
        "[Update the results on the Google Sheet](https://docs.google.com/spreadsheets/d/1tFsScPgzPsGqZCGhDM3cJgXysmp_lce1kUN76B2Rha8/edit?usp=sharing)\n",
        "\n",
        "### Aim for Final Version\n",
        "\n",
        "- Table of Vectorisation Methods * Classification Method * |Additional Methods| - Table of accuracies for different combinations of analysis methods as detailed in _Table of Analyses.xlsx_ (see `organisation/` directory)\n",
        "- Explanation/Exposition of methods\n",
        "- EDA - Visualise vector word representations out of different pre-processing; Basic descriptive statistics on final _input dataset_\n",
        "- Clear and clean pre-processing pipeline\n",
        "- Clear and clean grid search methods\n",
        "\n",
        "### Modelling Combinations\n",
        "\n",
        "#### Pre-processing\n",
        "\n",
        "- Components (methods) of `CleanText`\n",
        "    - In particular stemming\n",
        "\n",
        "#### Word Representations\n",
        "\n",
        "- Bag-of-Words - One-Hot (BOW)\n",
        "    - BOW n-grams with $n > 1$\n",
        "- Bag-of-Words - Frequencies (FBOW)\n",
        "- Term Frequency–Inverse Document Frequency (TF-IDF)\n",
        "- Word2Vec\n",
        "    - Skip-grams (SG)\n",
        "    - Continuous-Bag-of-Words CBOW\n",
        "- FastText\n",
        "- Bert\n",
        "\n",
        "#### Classifiers\n",
        "\n",
        "- Logistic Regression (Elastic Net)\n",
        "    - Search across penalisation weights (C) and l1-l2 ratios (l1_ratio)\n",
        "- Support Vector Machines (SVM)\n",
        "- Naive Bayes (NB)\n",
        "- Random Forests (RF)\n",
        "- Gradient Boosting (GB)\n",
        "- (Perceptron) (MLP)\n",
        "\n",
        "#### Additional Modelling Considerations\n",
        "\n",
        "- Scaled versus Unscaled data\n",
        "\n",
        "### Questions for the Team\n",
        "\n",
        "- Logistic Regression: Thoughts on mean accuracy as given by `LogisticRegression.score()`?\n",
        "- Logistic regression was fitted on individual words previously in Felicie's notebok. Given that we have a limited number of accounts, people might have tendencies to use the same words so our low train and test errors might come from here. Stemming reduces the training and test accuracies. **We should check if the components explaining a high degree of variation are individual words used by certain accounts.**\n",
        "- What should our cut-off for the minimum _document frequency_ of words be. The value 10 has been used, but 1 is the default with `CountVectorizer`.\n",
        "\n",
        "### Messages to the Team\n",
        "\n",
        "- The repo is public - This allows us to directly read data in by passing Pandas a URL\n",
        "- You can run code via AWS as if you're working locally. Follow [this tutorial](https://chrisalbon.com/aws/basics/run_project_jupyter_on_amazon_ec2/).\n",
        "\n",
        "### TODO\n",
        "\n",
        "- How has Felicie's approach to BOW vectorisation been realised by `sklearn.feature_extraction.text.CountVectorizer`? In particular, has any stemming been performed and if so via which algorithm? See the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
        "- Which dimensions explain the most variation? Inspect model coefficients; run PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF6qZ7Q-wSaX",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6IYM4mwwSaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start Fresh\n",
        "\n",
        "%reset -f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfgzfiZgwfbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dc2226ad-8e52-4261-e8d5-44aa6fcb8146"
      },
      "source": [
        "pip install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-dMZ87uISaz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "c503d8f6-79df-4ff5-b2dc-f029b2de8b0a"
      },
      "source": [
        "pip install catboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/aa/e61819d04ef2bbee778bf4b3a748db1f3ad23512377e43ecfdc3211437a0/catboost-0.23.2-cp36-none-manylinux1_x86_64.whl (64.8MB)\n",
            "\u001b[K     |████████████████████████████████| 64.8MB 75kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.18.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.23.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r62kDf7wSaq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fe4ba11d-dc0d-4585-b1f8-51bb54ffe969"
      },
      "source": [
        "# Imports and Set Options\n",
        "\n",
        "import csv  # for slang\n",
        "import os\n",
        "import re  # regex\n",
        "import string  # punct\n",
        "from pprint import pprint\n",
        "\n",
        "import emoji  # for emoji\n",
        "import gensim\n",
        "import keras\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from gensim.models import Word2Vec\n",
        "from IPython.display import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.corpus import stopwords  # stopwords\n",
        "from nltk.stem import PorterStemmer  # stemming\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import svm, tree\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
        "                              GradientBoostingClassifier,\n",
        "                              RandomForestClassifier, RandomForestRegressor,\n",
        "                              StackingClassifier)\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import (accuracy_score, auc, average_precision_score,\n",
        "                             brier_score_loss, classification_report,\n",
        "                             confusion_matrix, f1_score, fbeta_score,\n",
        "                             make_scorer, plot_precision_recall_curve,\n",
        "                             precision_recall_curve, precision_score,\n",
        "                             recall_score, roc_auc_score, roc_curve)\n",
        "from sklearn.model_selection import (GridSearchCV, KFold, RandomizedSearchCV,\n",
        "                                     cross_val_score, train_test_split)\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
        "from sklearn.svm import SVC  # \"Support vector classifier\"\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "import catboost as cb\n",
        "import xgboost as xgb\n",
        "\n",
        "# pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kedGb6ncwSaz",
        "colab_type": "text"
      },
      "source": [
        "## Homemade Classes and Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EuumKUFwSa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean Text Class\n",
        "\n",
        "class CleanText(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    def remove_mentions(self, input_text):\n",
        "        '''\n",
        "        Remove mentions, like @Mplamplampla\n",
        "        '''\n",
        "        return re.sub(r'@+', '', input_text)\n",
        "    \n",
        "    def remove_urls(self, input_text):\n",
        "        '''\n",
        "        Remove the urls mention in a tweet\n",
        "        '''\n",
        "        input_text  = ' '.join([w for w in input_text.split(' ') if '.com' not in w])\n",
        "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
        "    \n",
        "    def emoji_oneword(self, input_text):\n",
        "        # By compressing the underscore, the emoji is kept as one word\n",
        "        input_text = emoji.demojize(input_text)\n",
        "        input_text = input_text.replace('_','')\n",
        "        input_text = input_text.replace(':','')\n",
        "        return input_text\n",
        "    \n",
        "    def possessive_pronouns(self, input_text):\n",
        "        '''\n",
        "        Remove the possesive pronouns, because otherwise after tokenization we will end up with a word and an s\n",
        "        Example: government's --> [\"government\", \"s\"]\n",
        "        '''\n",
        "        return input_text.replace(\"'s\", \"\")\n",
        "    \n",
        "    def characters(self, input_text):\n",
        "        '''\n",
        "        Remove special and redundant characters that may appear on a tweet and that don't really help in our analysis\n",
        "        '''\n",
        "        input_text = input_text.replace(\"\\r\", \" \") # Carriage Return\n",
        "        input_text = input_text.replace(\"\\n\", \" \") # Newline\n",
        "        input_text = \" \".join(input_text.split()) # Double space\n",
        "        input_text = input_text.replace('\"', '') # Quotes\n",
        "        return input_text\n",
        "    \n",
        "    def remove_punctuation(self, input_text):\n",
        "        '''\n",
        "        Remove punctuation and specifically these symbols '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "        '''\n",
        "        punct = string.punctuation # string with all the punctuation symbols '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
        "        return input_text.translate(trantab)\n",
        "    \n",
        "    def remove_digits(self, input_text):\n",
        "        '''\n",
        "        Remove numbers\n",
        "        '''\n",
        "        return re.sub('\\d+', '', input_text)\n",
        "    \n",
        "    def to_lower(self, input_text):\n",
        "        '''\n",
        "        Convert all the sentences(words) to lowercase\n",
        "        '''\n",
        "        return input_text.lower()\n",
        "    \n",
        "    def remove_stopwords(self, input_text):\n",
        "        '''\n",
        "        Remove stopwords (refers to the most common words in a language)\n",
        "        '''\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "        whitelist = [\"n't\", \"not\", \"no\"]\n",
        "        words = input_text.split() \n",
        "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "        return \" \".join(clean_words) \n",
        "    \n",
        "    def stemming(self, input_text):\n",
        "        '''\n",
        "        Reduce the words to their stem\n",
        "        '''\n",
        "        porter = PorterStemmer()\n",
        "        words = input_text.split() \n",
        "        stemmed_words = [porter.stem(word) for word in words]\n",
        "        return \" \".join(stemmed_words)\n",
        "    \n",
        "    def encode_decode(self, input_text):\n",
        "        '''\n",
        "        Remove weird characters that are result of encoding problems\n",
        "        '''\n",
        "        return  \" \".join([k.encode(\"ascii\", \"ignore\").decode() for k in input_text.split(\" \")])\n",
        "    \n",
        "    \n",
        "    def translator(self, input_text):\n",
        "        '''\n",
        "        Transform abbrevations to normal words\n",
        "        Example: asap --> as soon as possible\n",
        "        '''\n",
        "        input_text = input_text.split(\" \")\n",
        "        j = 0\n",
        "        for _str in input_text:\n",
        "            # File path which consists of Abbreviations.\n",
        "            fileName = r\"slang.txt\"\n",
        "            # File Access mode [Read Mode]\n",
        "            accessMode = \"r\"\n",
        "            with open(fileName, accessMode) as myCSVfile:\n",
        "                # Reading file as CSV with delimiter as \"=\", so that \n",
        "                # abbreviation are stored in row[0] and phrases in row[1]\n",
        "                dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
        "                # Removing Special Characters.\n",
        "                _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
        "                for row in dataFromFile:\n",
        "                    # Check if selected word matches short forms[LHS] in text file.\n",
        "                    if _str.upper() == row[0]:\n",
        "                        # If match found replace it with its appropriate phrase in text file.\n",
        "                        input_text[j] = row[1]\n",
        "                myCSVfile.close()\n",
        "            j = j + 1\n",
        "        \n",
        "        return(' '.join(input_text))\n",
        "    \n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, **transform_params):\n",
        "        clean_X = (X.apply(self.translator)\n",
        "                    .apply(self.remove_mentions)\n",
        "                    .apply(self.remove_urls)\n",
        "                    .apply(self.emoji_oneword)\n",
        "                    .apply(self.possessive_pronouns)\n",
        "                    .apply(self.remove_punctuation)\n",
        "                    .apply(self.remove_digits)\n",
        "                    .apply(self.encode_decode)\n",
        "                    .apply(self.characters)\n",
        "                    .apply(self.to_lower)\n",
        "                    .apply(self.remove_stopwords)\n",
        "                    .apply(self.stemming))\n",
        "        return clean_X\n",
        "    \n",
        "    def transform_no_stem(self, X, **transform_params):\n",
        "        clean_X = (X.apply(self.translator)\n",
        "                    .apply(self.remove_mentions)\n",
        "                    .apply(self.remove_urls)\n",
        "                    .apply(self.emoji_oneword)\n",
        "                    .apply(self.possessive_pronouns)\n",
        "                    .apply(self.remove_punctuation)\n",
        "                    .apply(self.remove_digits)\n",
        "                    .apply(self.encode_decode)\n",
        "                    .apply(self.characters)\n",
        "                    .apply(self.to_lower)\n",
        "                    .apply(self.remove_stopwords))\n",
        "        return clean_X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLYwlItywSa7",
        "colab_type": "text"
      },
      "source": [
        "## Read in Data and Create Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90XNHjTKwSa8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9024d46-dc5c-4a0b-d781-1023e75042a5"
      },
      "source": [
        "# Read in data (Raw copy for reference; copy for processing)\n",
        "\n",
        "tweets_raw = pd.read_csv('https://github.com/anilkeshwani/StatLearnProj/raw/master/Iason/climate_change_tweets_sample-2020-05-16-17-57.csv')\n",
        "tweets = pd.read_csv('https://github.com/anilkeshwani/StatLearnProj/raw/master/Iason/climate_change_tweets_sample-2020-05-16-17-57.csv')\n",
        "tweets.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>user_handle</th>\n",
              "      <th>date</th>\n",
              "      <th>retweets</th>\n",
              "      <th>favorites</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-04-28</td>\n",
              "      <td>11</td>\n",
              "      <td>22</td>\n",
              "      <td>Economic recovery and national climate pledges...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-04-22</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>In this difficult time, it’s hard to connect w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-04-01</td>\n",
              "      <td>43</td>\n",
              "      <td>69</td>\n",
              "      <td>The decision to postpone # COP26, is unavoidab...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-03-30</td>\n",
              "      <td>24</td>\n",
              "      <td>30</td>\n",
              "      <td>Japan - the world’s fifth largest emitter of g...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-03-30</td>\n",
              "      <td>22</td>\n",
              "      <td>40</td>\n",
              "      <td>How can countries include # NatureBasedSolutio...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               username user_handle        date retweets favorites                                               text  label\n",
              "0  WWF Climate & Energy  climateWWF  2020-04-28       11        22  Economic recovery and national climate pledges...      0\n",
              "1  WWF Climate & Energy  climateWWF  2020-04-22        6        16  In this difficult time, it’s hard to connect w...      0\n",
              "2  WWF Climate & Energy  climateWWF  2020-04-01       43        69  The decision to postpone # COP26, is unavoidab...      0\n",
              "3  WWF Climate & Energy  climateWWF  2020-03-30       24        30  Japan - the world’s fifth largest emitter of g...      0\n",
              "4  WWF Climate & Energy  climateWWF  2020-03-30       22        40  How can countries include # NatureBasedSolutio...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AO4GN0iwSbD",
        "colab_type": "text"
      },
      "source": [
        "## Clean Dataset\n",
        "\n",
        "Applies the class methods (leveraging `sklearn` API):\n",
        "\n",
        "- translator\n",
        "- remove_mentions\n",
        "- remove_urls\n",
        "- emoji_oneword\n",
        "- possessive_pronouns\n",
        "- remove_punctuation\n",
        "- remove_digits\n",
        "- encode_decode\n",
        "- characters\n",
        "- to_lower\n",
        "- remove_stopwords\n",
        "- stemming (via Porter Algorithm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkyWe4MlwSbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Text Cleaning\n",
        "\n",
        "# ct = CleanText()\n",
        "# tweets[\"text\"] = ct.fit_transform(tweets[\"text\"])\n",
        "# tweets.to_csv(\"clean_tweets.csv\") # save once processed\n",
        "tweets = pd.read_csv(\"clean_tweets.csv\") # read in instead\n",
        "tweets = tweets.loc[(~tweets.text.isnull()), :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N222R0K7wSbK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17083149-f7a0-4747-d2d6-04f30056e2a9"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(tweets.text, tweets.label, \n",
        "                                                    test_size=0.2, random_state=17, \n",
        "                                                    shuffle=True) # explicit default\n",
        "\n",
        "[print(dat.head(3), dat.shape, end=\"\\n\"*2) for dat in [X_train, X_test, Y_train, Y_test]];"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3642     might progress climat area not enough global s...\n",
            "12695    trump crackdown politic scienc nasa climat div...\n",
            "8451     no one would believ human panick fals climat m...\n",
            "Name: text, dtype: object (14406,)\n",
            "\n",
            "8376     nazi root environment climat chang fraud bbcne...\n",
            "6111     interest democrat candid compar mani aspect cl...\n",
            "13983    ittrademark imposs see global warm signal minn...\n",
            "Name: text, dtype: object (3602,)\n",
            "\n",
            "3642     0\n",
            "12695    1\n",
            "8451     1\n",
            "Name: label, dtype: int64 (14406,)\n",
            "\n",
            "8376     1\n",
            "6111     0\n",
            "13983    1\n",
            "Name: label, dtype: int64 (3602,)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDGlEnHLwSbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7600dc-4df4-4820-d0b7-b48dab312665"
      },
      "source": [
        "print(f\"Training label counts: \\n{Y_train.value_counts()}\", end=\"\\n\"*2)\n",
        "print(f\"Test label counts: \\n{Y_test.value_counts()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training label counts: \n",
            "1    8433\n",
            "0    5973\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Test label counts: \n",
            "1    2138\n",
            "0    1464\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7eLj2v9wSbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save set of workspace objects' names to enable periodic clean-up\n",
        "\n",
        "necessities = set(dir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0YQ97WswSbZ",
        "colab_type": "text"
      },
      "source": [
        "## Word Vectorisations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxhIvVpTwSbb",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Words (BOW) Binary (\"One-Hot\") Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4UOs2r1wSbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bag of Words Representation (One Hot, i.e. binary)\n",
        "\n",
        "BOW_vectorizer = CountVectorizer(stop_words = 'english', \n",
        "                                 binary=True, # Creates 0/1 \"One Hot\" vector; \n",
        "                                              # np.unique(BOW_train.toarray())\n",
        "                                 min_df = 10)\n",
        "BOW_vectorizer.fit(X_train)\n",
        "BOW_train = BOW_vectorizer.transform(X_train)\n",
        "BOW_test = BOW_vectorizer.transform(X_test)\n",
        "\n",
        "# Construct Scaled Datasets\n",
        "\n",
        "scaler_BOW = MaxAbsScaler()\n",
        "BOW_train_scaled = scaler_BOW.fit_transform(BOW_train)\n",
        "BOW_test_scaled = scaler_BOW.transform(BOW_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7uF40fwwSbi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced938b3-a596-4766-e89e-9237cdc90d80"
      },
      "source": [
        "# Most frequently occurring words in the training corpus\n",
        "\n",
        "[(index, word) for index, word in sorted(BOW_vectorizer.vocabulary_.items(), key=lambda item: item[1], reverse=True)][:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('zero', 2206),\n",
              " ('yr', 2205),\n",
              " ('youtub', 2204),\n",
              " ('youthvgov', 2203),\n",
              " ('youthtopow', 2202),\n",
              " ('youthclimatesummit', 2201),\n",
              " ('youth', 2200),\n",
              " ('young', 2199),\n",
              " ('york', 2198),\n",
              " ('yesterday', 2197),\n",
              " ('year', 2196),\n",
              " ('yeah', 2195),\n",
              " ('ye', 2194),\n",
              " ('yall', 2193),\n",
              " ('yale', 2192),\n",
              " ('ya', 2191),\n",
              " ('wwf', 2190),\n",
              " ('wsj', 2189),\n",
              " ('wrote', 2188),\n",
              " ('wrong', 2187)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfz9mb-iwSbm",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Words with Frequencies Representation (FBOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHPu55NZwSbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bag of Words Representation (Frequencies; binary=False)\n",
        "\n",
        "FBOW_vectorizer = CountVectorizer(stop_words = 'english', \n",
        "                                  binary=False, # Creates Word Frequency Vector; \n",
        "                                                # # np.unique(FBOW_train.toarray())\n",
        "                                  min_df = 10)\n",
        "FBOW_vectorizer.fit(X_train)\n",
        "FBOW_train = FBOW_vectorizer.transform(X_train)\n",
        "FBOW_test = FBOW_vectorizer.transform(X_test)\n",
        "\n",
        "# Construct Scaled Datasets\n",
        "\n",
        "scaler_FBOW = MaxAbsScaler()\n",
        "FBOW_train_scaled = scaler_FBOW.fit_transform(FBOW_train)\n",
        "FBOW_test_scaled = scaler_FBOW.transform(FBOW_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMQCDGVnwSbt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24019c55-a578-440c-9554-9867039bf0dd"
      },
      "source": [
        "# Word use (per tweet) frequencies\n",
        "\n",
        "print(np.unique(FBOW_train.toarray(), return_counts=True))\n",
        "\n",
        "# Feature_Index: Word Mapping\n",
        "\n",
        "# {v: k for k, v in FBOW_vectorizer.vocabulary_.items()}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 13]), array([31661370,   127070,     5161,      379,       45,        7,\n",
            "              5,        1,        2,        1,        1]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Deku25vLwSb5",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Words Bigrams (bigram)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aWw91FSwSb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigram_vectorizer = CountVectorizer(stop_words = 'english', \n",
        "                                    binary=True, \n",
        "                                    min_df = 10,\n",
        "                                    ngram_range = (1,2)) # create bigrams\n",
        "bigram_vectorizer.fit(X_train)\n",
        "\n",
        "bigram_train = bigram_vectorizer.transform(X_train)\n",
        "bigram_test = bigram_vectorizer.transform(X_test)\n",
        "\n",
        "# Construct Scaled Datasets\n",
        "\n",
        "scaler_bigram = MaxAbsScaler()\n",
        "bigram_train_scaled = scaler_bigram.fit_transform(bigram_train)\n",
        "bigram_test_scaled = scaler_bigram.transform(bigram_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNMC9UM7wSb0",
        "colab_type": "text"
      },
      "source": [
        "### Term Frequency–Inverse Document Frequency Representation (tf-idf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVMWBo0TwSb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', \n",
        "                                   min_df=10) # used for now for consistency\n",
        "tfidf_vectorizer.fit(X_train)\n",
        "tfidf_train = tfidf_vectorizer.transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Construct Scaled Datasets\n",
        "\n",
        "scaler_tfidf = MaxAbsScaler()\n",
        "tfidf_train_scaled = scaler_tfidf.fit_transform(tfidf_train)\n",
        "tfidf_test_scaled = scaler_tfidf.transform(tfidf_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfp0XIiIwSb-",
        "colab_type": "text"
      },
      "source": [
        "### Word2Vec - Continuous-Bag-of-Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNx7srbPwSb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MpoIwi1I-l7",
        "colab_type": "text"
      },
      "source": [
        "## Grid-Searches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQeQp6c9wtVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def var_name(var):\n",
        "    for name,value in globals().items() :\n",
        "        if value is var :\n",
        "            return name\n",
        "    return '?????' \n",
        " \n",
        "def printv(var):\n",
        "    print(\"##\",var_name(var))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c91AfGKaPBzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfcv = KFold(n_splits=5,shuffle=True,random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uE0f8-OO2bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GS(X):\n",
        "\n",
        "  ### X TRAIN\n",
        "  X_train = X.toarray()\n",
        "\n",
        "  ### Gaussian Naives Bayes\n",
        "  clf = GaussianNB()\n",
        "  var_smoothing = [pow(10,k)/1000000000 for k in range(10)]\n",
        "  param_grid = {\n",
        "           'var_smoothing': var_smoothing\n",
        "        }\n",
        "  grid_search_NB = GridSearchCV(estimator = clf, param_grid = param_grid, cv=kfcv ,n_jobs = -1, verbose = 2, scoring='accuracy')\n",
        "  grid_search_NB.fit(X_train,Y_train)\n",
        "\n",
        "  ### Logistic regression\n",
        "  clf = LogisticRegression()\n",
        "  param_grid = {\n",
        "            'penalty': ['elasticnet','l1','l2','none'],\n",
        "            'C': [.001, .01, .1, 1, 10, 100, 1000],\n",
        "            'solver': ['liblinear', \"saga\", \"lbfgs\", \"newton-cg\", \"sag\"],\n",
        "            'multi_class': ['ovr'],\n",
        "            'max_iter' : [1000]\n",
        "        }\n",
        "  grid_search_LR = GridSearchCV(estimator = clf, param_grid = param_grid, cv=kfcv ,n_jobs = -1, verbose = 2)\n",
        "  grid_search_LR.fit(X_train,Y_train)\n",
        "\n",
        "  ### Random Forest\n",
        "  clf = RandomForestClassifier(oob_score=True)\n",
        "  param_grid = {\n",
        "    'bootstrap': [True],\n",
        "    'max_depth': [80, 100, None],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'min_samples_leaf': [1, 3, 5],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'n_estimators': [50, 100, 300]\n",
        "  }\n",
        "  grid_search_RF = GridSearchCV(estimator = clf, param_grid = param_grid, cv=kfcv ,n_jobs = -1, verbose = 2)\n",
        "  grid_search_RF.fit(X_train,Y_train)\n",
        "\n",
        "  ### SuperVectorMachine\n",
        "  clf = SVC()\n",
        "  param_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
        "                     'C': [1,2,10,100]},\n",
        "                    {'kernel': ['linear'], 'C': [1,2,10,100]}]\n",
        "  grid_search_SVM = GridSearchCV(estimator = clf, param_grid = param_grid, cv=kfcv ,n_jobs = -1, verbose = 2, scoring='accuracy')\n",
        "  grid_search_SVM.fit(X_train,Y_train)\n",
        "\n",
        "  ### Yandex CatBoost\n",
        "  clf = cb.CatBoostClassifier(random_state=17,thread_count=4,verbose=0)\n",
        "  param_grid = {'n_estimators':[100,250,500,1000],\n",
        "              'depth':sp_randint(1,10),\n",
        "              'learning_rate':[0.001,0.01,0.05,0.1,0.2,0.3], \n",
        "              'l2_leaf_reg':[1,5,10,100],\n",
        "              'border_count':[5,10,20,50,100,200]}\n",
        "  grid_search_CB = GridSearchCV(estimator = clf, param_grid = param_grid, cv=kfcv ,n_jobs = -1, verbose = 2, scoring='accuracy')\n",
        "  grid_search_CB.fit(X_train,Y_train)\n",
        "  \n",
        "  ### Print the best parameters\n",
        "  print(\"#####################################\\n#####################################\")\n",
        "  printv(X)\n",
        "  print(\"#####################################\\n##\")\n",
        "  print(\"## Best Naive Bayes parameters :\",grid_search_NB.best_params_)\n",
        "  print(\"## Best Logistic Regression parameters :\", grid_search_LR.best_params_)\n",
        "  print(\"## Best Random Forest parameters :\", grid_search_RF.best_params_)\n",
        "  print(\"## Best Super Vector Machines parameters :\", grid_search_SVM.best_params_)\n",
        "  print(\"## Best Cat Boost parameters :\", grid_search_CB.best_params_)\n",
        "  print(\"##\\n#####################################\\n#####################################\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub4A0-2DJaC4",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Word NO SCALED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWRGmjifQPlc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "323737cb-88d0-4b84-ace7-1dbe65a15d20"
      },
      "source": [
        "GS(BOW_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   19.1s\n",
            "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   24.8s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 140 candidates, totalling 700 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    9.7s\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rHCStQOERYt",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Word SCALED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY8dk8zMEWIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GS(BOW_train_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZdjuHM4EoqV",
        "colab_type": "text"
      },
      "source": [
        "### Frequency Bag of Word NO SCALED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4losCyiEtxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GS(FBOW_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDStGEKkE7YF",
        "colab_type": "text"
      },
      "source": [
        "### Frequency Bag of Word SCALED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEV_L96GE-tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GS(FBOW_train_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vVvwgmYF0Wm",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Word BIGRAM NO SCALED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSQVuFkgF6Vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GS(bigram_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T4iAIMlGRNy",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Word BIGRAM SCALED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9yf1yq8G4zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GS(bigram_train_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwTP7f0HLFTQ",
        "colab_type": "text"
      },
      "source": [
        "### TF IDF NO SCALED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xOdNvZULLL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GS(tfidf_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdkpa24FLdZ0",
        "colab_type": "text"
      },
      "source": [
        "### TF IDF SCALED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_155lA0QLg9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GS(tfidf_train_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}