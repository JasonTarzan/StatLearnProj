{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clean_Text.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUmZD8vtjGyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start Fresh\n",
        " \n",
        "%reset -f\n",
        "! pip install emoji\n",
        "! pip install catboost\n",
        "import csv  # for slang\n",
        "import os\n",
        "import re  # regex\n",
        "import string  # punct\n",
        "from pprint import pprint\n",
        " \n",
        "import emoji  # for emoji\n",
        "import gensim\n",
        "import keras\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm_notebook\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from gensim.models import Word2Vec\n",
        "from IPython.display import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.corpus import stopwords  # stopwords\n",
        "from nltk.stem import PorterStemmer  # stemming\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import svm, tree\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
        "                              GradientBoostingClassifier,\n",
        "                              RandomForestClassifier, RandomForestRegressor,\n",
        "                              StackingClassifier)\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import (accuracy_score, auc, average_precision_score,\n",
        "                             brier_score_loss, classification_report,\n",
        "                             confusion_matrix, f1_score, fbeta_score,\n",
        "                             make_scorer, plot_precision_recall_curve,\n",
        "                             precision_recall_curve, precision_score,\n",
        "                             recall_score, roc_auc_score, roc_curve)\n",
        "from sklearn.model_selection import (GridSearchCV, KFold, RandomizedSearchCV,\n",
        "                                     cross_val_score, train_test_split)\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
        "from sklearn.svm import SVC  # \"Support vector classifier\"\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        " \n",
        "import catboost as cb\n",
        "import xgboost as xgb\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import uniform as sp_uniform\n",
        " \n",
        "# pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from operator import itemgetter\n",
        "\n",
        " \n",
        "%matplotlib inline\n",
        "from IPython.display import clear_output\n",
        " \n",
        "from google.colab import files # to read slang.txt until better solution is foun\n",
        "uploaded = files.upload()\n",
        "import time\n",
        "start_time = time.time()\n",
        "clear_output()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdrY9gM8jK2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean Text Class\n",
        "\n",
        "class CleanText(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    def remove_mentions(self, input_text):\n",
        "        '''\n",
        "        Remove mentions, like @Mplamplampla\n",
        "        '''\n",
        "        return re.sub(r'@+', '', input_text)\n",
        "    \n",
        "    def remove_urls(self, input_text):\n",
        "        '''\n",
        "        Remove the urls mention in a tweet\n",
        "        '''\n",
        "        input_text  = ' '.join([w for w in input_text.split(' ') if '.com' not in w])\n",
        "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
        "    \n",
        "    def emoji_oneword(self, input_text):\n",
        "        # By compressing the underscore, the emoji is kept as one word\n",
        "        input_text = emoji.demojize(input_text)\n",
        "        input_text = input_text.replace('_','')\n",
        "        input_text = input_text.replace(':','')\n",
        "        return input_text\n",
        "    \n",
        "    def possessive_pronouns(self, input_text):\n",
        "        '''\n",
        "        Remove the possesive pronouns, because otherwise after tokenization we will end up with a word and an s\n",
        "        Example: government's --> [\"government\", \"s\"]\n",
        "        '''\n",
        "        return input_text.replace(\"'s\", \"\")\n",
        "    \n",
        "    def characters(self, input_text):\n",
        "        '''\n",
        "        Remove special and redundant characters that may appear on a tweet and that don't really help in our analysis\n",
        "        '''\n",
        "        input_text = input_text.replace(\"\\r\", \" \") # Carriage Return\n",
        "        input_text = input_text.replace(\"\\n\", \" \") # Newline\n",
        "        input_text = \" \".join(input_text.split()) # Double space\n",
        "        input_text = input_text.replace('\"', '') # Quotes\n",
        "        return input_text\n",
        "    \n",
        "    def remove_punctuation(self, input_text):\n",
        "        '''\n",
        "        Remove punctuation and specifically these symbols '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "        '''\n",
        "        punct = string.punctuation # string with all the punctuation symbols '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
        "        return input_text.translate(trantab)\n",
        "    \n",
        "    def remove_digits(self, input_text):\n",
        "        '''\n",
        "        Remove numbers\n",
        "        '''\n",
        "        return re.sub('\\d+', '', input_text)\n",
        "    \n",
        "    def to_lower(self, input_text):\n",
        "        '''\n",
        "        Convert all the sentences(words) to lowercase\n",
        "        '''\n",
        "        return input_text.lower()\n",
        "    \n",
        "    def remove_stopwords(self, input_text):\n",
        "        '''\n",
        "        Remove stopwords (refers to the most common words in a language)\n",
        "        '''\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "        whitelist = [\"n't\", \"not\", \"no\"]\n",
        "        words = input_text.split() \n",
        "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "        return \" \".join(clean_words) \n",
        "    \n",
        "    def stemming(self, input_text):\n",
        "        '''\n",
        "        Reduce the words to their stem\n",
        "        '''\n",
        "        porter = PorterStemmer()\n",
        "        words = input_text.split() \n",
        "        stemmed_words = [porter.stem(word) for word in words]\n",
        "        return \" \".join(stemmed_words)\n",
        "    \n",
        "    def encode_decode(self, input_text):\n",
        "        '''\n",
        "        Remove weird characters that are result of encoding problems\n",
        "        '''\n",
        "        return  \" \".join([k.encode(\"ascii\", \"ignore\").decode() for k in input_text.split(\" \")])\n",
        "    \n",
        "    \n",
        "    def translator(self, input_text):\n",
        "        '''\n",
        "        Transform abbrevations to normal words\n",
        "        Example: asap --> as soon as possible\n",
        "        '''\n",
        "        input_text = input_text.split(\" \")\n",
        "        j = 0\n",
        "        for _str in input_text:\n",
        "            # File path which consists of Abbreviations.\n",
        "            fileName = \"/content/slang.txt\"\n",
        "            # File Access mode [Read Mode]\n",
        "            accessMode = \"r\"\n",
        "            with open(fileName, accessMode) as myCSVfile:\n",
        "                # Reading file as CSV with delimiter as \"=\", so that \n",
        "                # abbreviation are stored in row[0] and phrases in row[1]\n",
        "                dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
        "                # Removing Special Characters.\n",
        "                _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
        "                for row in dataFromFile:\n",
        "                    # Check if selected word matches short forms[LHS] in text file.\n",
        "                    if _str.upper() == row[0]:\n",
        "                        # If match found replace it with its appropriate phrase in text file.\n",
        "                        input_text[j] = row[1]\n",
        "                myCSVfile.close()\n",
        "            j = j + 1\n",
        "        \n",
        "        return(' '.join(input_text))\n",
        "    \n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, **transform_params):\n",
        "        clean_X = (X.apply(self.translator)\n",
        "                    .apply(self.remove_mentions)\n",
        "                    .apply(self.remove_urls)\n",
        "                    .apply(self.emoji_oneword)\n",
        "                    .apply(self.possessive_pronouns)\n",
        "                    .apply(self.remove_punctuation)\n",
        "                    .apply(self.remove_digits)\n",
        "                    .apply(self.encode_decode)\n",
        "                    .apply(self.characters)\n",
        "                    .apply(self.to_lower)\n",
        "                    .apply(self.remove_stopwords)\n",
        "                    .apply(self.stemming))\n",
        "        return clean_X\n",
        "    \n",
        "    def transform_no_stem(self, X, **transform_params):\n",
        "        clean_X = (X.apply(self.translator)\n",
        "                    .apply(self.remove_mentions)\n",
        "                    .apply(self.remove_urls)\n",
        "                    .apply(self.emoji_oneword)\n",
        "                    .apply(self.possessive_pronouns)\n",
        "                    .apply(self.remove_punctuation)\n",
        "                    .apply(self.remove_digits)\n",
        "                    .apply(self.encode_decode)\n",
        "                    .apply(self.characters)\n",
        "                    .apply(self.to_lower)\n",
        "                    .apply(self.remove_stopwords))\n",
        "        return clean_X"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsqUHHZQjU7W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c166571a-4e21-49a1-ad95-d39592f7e762"
      },
      "source": [
        "our_text = \"I just wanted to say that @Aladdin you've been an inspiration for my webpage http://www.iloveAladdin.com. Also i ❤️ Jasmin's beautiful 24K  ring. \\nNot to complain but i need one asap  Ã¥\"\n",
        "print(our_text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I just wanted to say that @Aladdin you've been an inspiration for my webpage http://www.iloveAladdin.com. Also i ❤️ Jasmin's beautiful 24K  ring. \n",
            "Not to complain but i need one asap  Ã¥\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1glSiX-nknaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = CleanText()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnr0J6RakvLb",
        "colab_type": "text"
      },
      "source": [
        "#Abbreviations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8COLxQnkubc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "382d65d4-d443-45fd-a789-b2d6b374a6d0"
      },
      "source": [
        "our_text = ct.translator(our_text)\n",
        "print(our_text) # asap ---> As Soon As Possible"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I just wanted to say that @Aladdin you've been an inspiration for my webpage http://www.iloveAladdin.com. Also i ❤️ Jasmin's beautiful 24K  ring. \n",
            "Not to complain but i need one As Soon As Possible  Ã¥\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONx-3ehzk-tj",
        "colab_type": "text"
      },
      "source": [
        "# Remove Mentions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yxQl52ak4Xv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5f5e595f-ecb9-486a-ab04-1e9a352b8512"
      },
      "source": [
        "our_text = ct.remove_mentions(our_text)\n",
        "print(our_text) # @Aladdin ---> Aladdin"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I just wanted to say that Aladdin you've been an inspiration for my webpage http://www.iloveAladdin.com. Also i ❤️ Jasmin's beautiful 24K  ring. \n",
            "Not to complain but i need one As Soon As Possible  Ã¥\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwIezOoVlJl8",
        "colab_type": "text"
      },
      "source": [
        "# Remove URLs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJDD4IYIlHjz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4ef3c51e-1088-4f38-d4cf-8ccdeae5bf12"
      },
      "source": [
        "our_text = ct.remove_urls(our_text)\n",
        "print(our_text) # http://www.iloveAladdin.com. ---> ____"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I just wanted to say that Aladdin you've been an inspiration for my webpage Also i ❤️ Jasmin's beautiful 24K  ring. \n",
            "Not to complain but i need one As Soon As Possible  Ã¥\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadxmJnLlUo0",
        "colab_type": "text"
      },
      "source": [
        "# Emoji Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfHedoPslOWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "eefe6bbc-c1bc-49e1-c387-a8404d998dae"
      },
      "source": [
        "our_text = ct.emoji_oneword(our_text)\n",
        "print(our_text) # ❤️ ---> redheart"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I just wanted to say that Aladdin you've been an inspiration for my webpage Also i redheart Jasmin's beautiful 24K  ring. \n",
            "Not to complain but i need one As Soon As Possible  Ã¥\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xb68tbGlfbx",
        "colab_type": "text"
      },
      "source": [
        "# Remove Possessive Pronouns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaMwpTPKlZ8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "aaeb4836-fede-42ef-b2f3-2ab6ec627192"
      },
      "source": [
        "our_text = ct.possessive_pronouns(our_text)\n",
        "print(our_text) # Jasmine's ---> Jasmine"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I just wanted to say that Aladdin you've been an inspiration for my webpage Also i redheart Jasmin beautiful 24K  ring. \n",
            "Not to complain but i need one As Soon As Possible  Ã¥\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp1VZNOTl57Z",
        "colab_type": "text"
      },
      "source": [
        "# Remove Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IkrvVIqlnd5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ae2264b5-6d0f-4964-b6f3-5d3062278566"
      },
      "source": [
        "our_text = ct.remove_punctuation(our_text)\n",
        "print(our_text) # [', .] ---> ____"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I just wanted to say that Aladdin you ve been an inspiration for my webpage Also i redheart Jasmin beautiful 24K  ring  \n",
            "Not to complain but i need one As Soon As Possible  Ã¥\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5ZQtBcAmHfO",
        "colab_type": "text"
      },
      "source": [
        "# Remove Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB3_XJnel9vw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3043a4a1-1119-4ae5-ab2b-d5201dae3993"
      },
      "source": [
        "our_text = ct.remove_digits(our_text)\n",
        "print(our_text) # 24K ---> K"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I just wanted to say that Aladdin you ve been an inspiration for my webpage Also i redheart Jasmin beautiful K  ring  \n",
            "Not to complain but i need one As Soon As Possible  Ã¥\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNOxStmpma0W",
        "colab_type": "text"
      },
      "source": [
        "# Remove Weird Characters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe40XmcnmLEA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c148bbcf-3f39-46d5-eeda-cd50391bc7cd"
      },
      "source": [
        "our_text = ct.encode_decode(our_text)\n",
        "print(our_text) # Ã¥ ---> ___"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I just wanted to say that Aladdin you ve been an inspiration for my webpage Also i redheart Jasmin beautiful K  ring  \n",
            "Not to complain but i need one As Soon As Possible  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkBHrobJml4V",
        "colab_type": "text"
      },
      "source": [
        "# Remove characters like tab, next line double space etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtwUFhVDmdhC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "4b36eb5c-d049-45e7-ac08-704dee0528b6"
      },
      "source": [
        "our_text = ct.characters(our_text)\n",
        "print(our_text) # \\n ---> ___"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I just wanted to say that Aladdin you ve been an inspiration for my webpage Also i redheart Jasmin beautiful K ring Not to complain but i need one As Soon As Possible\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt5nnOkzmx82",
        "colab_type": "text"
      },
      "source": [
        "# All the capital to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l29ShByHmsBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "1d167d0c-bed7-43f1-c051-4bec2d5e5b6d"
      },
      "source": [
        "our_text = ct.to_lower(our_text)\n",
        "print(our_text) # \\n ---> ___"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i just wanted to say that aladdin you ve been an inspiration for my webpage also i redheart jasmin beautiful k ring not to complain but i need one as soon as possible\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RSSLzXIm67K",
        "colab_type": "text"
      },
      "source": [
        "# Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EJxWRVUmw91",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f893ea5e-cb30-4425-8059-71dc1e75a939"
      },
      "source": [
        "our_text = ct.remove_stopwords(our_text)\n",
        "print(our_text) # [i, just, to, that, you, ve, been, an, for, my, k, but, i, as] ---> ___"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wanted say aladdin inspiration webpage also redheart jasmin beautiful ring not complain need one soon possible\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}