{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmqe3ybK0fIv",
        "colab_type": "text"
      },
      "source": [
        "# Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84Dhwum40hQM",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "979eef85-1eab-49b7-bb04-a03f4f765005"
      },
      "source": [
        "import csv  # for slang\n",
        "import os\n",
        "import re  # regex\n",
        "import string  # punct\n",
        "from pprint import pprint\n",
        "!pip install emoji --upgrade\n",
        "import emoji  # for emoji\n",
        "import gensim\n",
        "import keras\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from gensim.models import Word2Vec\n",
        "from IPython.display import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.corpus import stopwords  # stopwords\n",
        "from nltk.stem import PorterStemmer  # stemming\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import svm, tree\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
        "                              GradientBoostingClassifier,\n",
        "                              RandomForestClassifier, RandomForestRegressor,\n",
        "                              StackingClassifier)\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import (accuracy_score, auc, average_precision_score,\n",
        "                             brier_score_loss, classification_report,\n",
        "                             confusion_matrix, f1_score, fbeta_score,\n",
        "                             make_scorer, plot_precision_recall_curve,\n",
        "                             precision_recall_curve, precision_score,\n",
        "                             recall_score, roc_auc_score, roc_curve)\n",
        "from sklearn.model_selection import (GridSearchCV, KFold, RandomizedSearchCV,\n",
        "                                     cross_val_score, train_test_split)\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
        "from sklearn.svm import SVC  # \"Support vector classifier\"\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "# pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-09bad49e-4bdf-42da-9048-b8df4a42bf64\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-09bad49e-4bdf-42da-9048-b8df4a42bf64\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving slang.txt to slang.txt\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h5LZKnH0ohD",
        "colab_type": "text"
      },
      "source": [
        "# Homemade Classes and Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KrEMlEO0qN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean Text Class\n",
        "\n",
        "class CleanText(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    def remove_mentions(self, input_text):\n",
        "        '''\n",
        "        Remove mentions, like @Mplamplampla\n",
        "        '''\n",
        "        return re.sub(r'@+', '', input_text)\n",
        "    \n",
        "    def remove_urls(self, input_text):\n",
        "        '''\n",
        "        Remove the urls mention in a tweet\n",
        "        '''\n",
        "        input_text  = ' '.join([w for w in input_text.split(' ') if '.com' not in w])\n",
        "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
        "    \n",
        "    def emoji_oneword(self, input_text):\n",
        "        # By compressing the underscore, the emoji is kept as one word\n",
        "        input_text = emoji.demojize(input_text)\n",
        "        input_text = input_text.replace('_','')\n",
        "        input_text = input_text.replace(':','')\n",
        "        return input_text\n",
        "    \n",
        "    def possessive_pronouns(self, input_text):\n",
        "        '''\n",
        "        Remove the possesive pronouns, because otherwise after tokenization we will end up with a word and an s\n",
        "        Example: government's --> [\"government\", \"s\"]\n",
        "        '''\n",
        "        return input_text.replace(\"'s\", \"\")\n",
        "    \n",
        "    def characters(self, input_text):\n",
        "        '''\n",
        "        Remove special and redundant characters that may appear on a tweet and that don't really help in our analysis\n",
        "        '''\n",
        "        input_text = input_text.replace(\"\\r\", \" \") # Carriage Return\n",
        "        input_text = input_text.replace(\"\\n\", \" \") # Newline\n",
        "        input_text = \" \".join(input_text.split()) # Double space\n",
        "        input_text = input_text.replace('\"', '') # Quotes\n",
        "        return input_text\n",
        "    \n",
        "    def remove_punctuation(self, input_text):\n",
        "        '''\n",
        "        Remove punctuation and specifically these symbols '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "        '''\n",
        "        punct = string.punctuation # string with all the punctuation symbols '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
        "        return input_text.translate(trantab)\n",
        "    \n",
        "    def remove_digits(self, input_text):\n",
        "        '''\n",
        "        Remove numbers\n",
        "        '''\n",
        "        return re.sub('\\d+', '', input_text)\n",
        "    \n",
        "    def to_lower(self, input_text):\n",
        "        '''\n",
        "        Convert all the sentences(words) to lowercase\n",
        "        '''\n",
        "        return input_text.lower()\n",
        "    \n",
        "    def remove_stopwords(self, input_text):\n",
        "        '''\n",
        "        Remove stopwords (refers to the most common words in a language)\n",
        "        '''\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "        whitelist = [\"n't\", \"not\", \"no\"]\n",
        "        words = input_text.split() \n",
        "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "        return \" \".join(clean_words) \n",
        "    \n",
        "    def stemming(self, input_text):\n",
        "        '''\n",
        "        Reduce the words to their stem\n",
        "        '''\n",
        "        porter = PorterStemmer()\n",
        "        words = input_text.split() \n",
        "        stemmed_words = [porter.stem(word) for word in words]\n",
        "        return \" \".join(stemmed_words)\n",
        "    \n",
        "    def encode_decode(self, input_text):\n",
        "        '''\n",
        "        Remove weird characters that are result of encoding problems\n",
        "        '''\n",
        "        return  \" \".join([k.encode(\"ascii\", \"ignore\").decode() for k in input_text.split(\" \")])\n",
        "    \n",
        "    \n",
        "    def translator(self, input_text):\n",
        "        '''\n",
        "        Transform abbrevations to normal words\n",
        "        Example: asap --> as soon as possible\n",
        "        '''\n",
        "        input_text = input_text.split(\" \")\n",
        "        j = 0\n",
        "        for _str in input_text:\n",
        "            # File path which consists of Abbreviations.\n",
        "            fileName = \"/content/slang.txt\"\n",
        "            # File Access mode [Read Mode]\n",
        "            accessMode = \"r\"\n",
        "            with open(fileName, accessMode) as myCSVfile:\n",
        "                # Reading file as CSV with delimiter as \"=\", so that \n",
        "                # abbreviation are stored in row[0] and phrases in row[1]\n",
        "                dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
        "                # Removing Special Characters.\n",
        "                _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
        "                for row in dataFromFile:\n",
        "                    # Check if selected word matches short forms[LHS] in text file.\n",
        "                    if _str.upper() == row[0]:\n",
        "                        # If match found replace it with its appropriate phrase in text file.\n",
        "                        input_text[j] = row[1]\n",
        "                myCSVfile.close()\n",
        "            j = j + 1\n",
        "        \n",
        "        return(' '.join(input_text))\n",
        "    \n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, **transform_params):\n",
        "        clean_X = (X.apply(self.translator)\n",
        "                    .apply(self.remove_mentions)\n",
        "                    .apply(self.remove_urls)\n",
        "                    .apply(self.emoji_oneword)\n",
        "                    .apply(self.possessive_pronouns)\n",
        "                    .apply(self.remove_punctuation)\n",
        "                    .apply(self.remove_digits)\n",
        "                    .apply(self.encode_decode)\n",
        "                    .apply(self.characters)\n",
        "                    .apply(self.to_lower)\n",
        "                    .apply(self.remove_stopwords)\n",
        "                    .apply(self.stemming))\n",
        "        return clean_X\n",
        "    \n",
        "    def transform_no_stem(self, X, **transform_params):\n",
        "        clean_X = (X.apply(self.translator)\n",
        "                    .apply(self.remove_mentions)\n",
        "                    .apply(self.remove_urls)\n",
        "                    .apply(self.emoji_oneword)\n",
        "                    .apply(self.possessive_pronouns)\n",
        "                    .apply(self.remove_punctuation)\n",
        "                    .apply(self.remove_digits)\n",
        "                    .apply(self.encode_decode)\n",
        "                    .apply(self.characters)\n",
        "                    .apply(self.to_lower)\n",
        "                    .apply(self.remove_stopwords))\n",
        "        return clean_X"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcaEjOSW0wiE",
        "colab_type": "text"
      },
      "source": [
        "# Read in Data and Create Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoLqA6pm0r2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "a7b1c18b-080a-448b-b322-ce8b4f447a78"
      },
      "source": [
        "tweets = pd.read_csv('https://github.com/anilkeshwani/StatLearnProj/raw/master/Iason/climate_change_tweets_sample-2020-05-16-17-57.csv')\n",
        "tweets.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>user_handle</th>\n",
              "      <th>date</th>\n",
              "      <th>retweets</th>\n",
              "      <th>favorites</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-04-28</td>\n",
              "      <td>11</td>\n",
              "      <td>22</td>\n",
              "      <td>Economic recovery and national climate pledges...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-04-22</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>In this difficult time, it’s hard to connect w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-04-01</td>\n",
              "      <td>43</td>\n",
              "      <td>69</td>\n",
              "      <td>The decision to postpone # COP26, is unavoidab...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-03-30</td>\n",
              "      <td>24</td>\n",
              "      <td>30</td>\n",
              "      <td>Japan - the world’s fifth largest emitter of g...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WWF Climate &amp; Energy</td>\n",
              "      <td>climateWWF</td>\n",
              "      <td>2020-03-30</td>\n",
              "      <td>22</td>\n",
              "      <td>40</td>\n",
              "      <td>How can countries include # NatureBasedSolutio...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               username user_handle        date retweets favorites                                               text  label\n",
              "0  WWF Climate & Energy  climateWWF  2020-04-28       11        22  Economic recovery and national climate pledges...      0\n",
              "1  WWF Climate & Energy  climateWWF  2020-04-22        6        16  In this difficult time, it’s hard to connect w...      0\n",
              "2  WWF Climate & Energy  climateWWF  2020-04-01       43        69  The decision to postpone # COP26, is unavoidab...      0\n",
              "3  WWF Climate & Energy  climateWWF  2020-03-30       24        30  Japan - the world’s fifth largest emitter of g...      0\n",
              "4  WWF Climate & Energy  climateWWF  2020-03-30       22        40  How can countries include # NatureBasedSolutio...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNjyaEnx0zB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = CleanText()\n",
        "tweets[\"text\"] = ct.fit_transform(tweets.text)\n",
        "tweets = tweets.loc[(~tweets.text.isnull()), :]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPepcDvU01md",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "0a030698-8bf3-4aaf-d3f0-e5502c735573"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(tweets.text, tweets.label, \n",
        "                                                    test_size=0.2, random_state=17, \n",
        "                                                    shuffle=True) # explicit default\n",
        "\n",
        "[print(dat.head(3), dat.shape, end=\"\\n\"*2) for dat in [X_train, X_test, Y_train, Y_test]];"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3641     insect integr part life environ increas heat c...\n",
            "11837                                  believ climat chang\n",
            "8450              climat model need worri programm set run\n",
            "Name: text, dtype: object (14407,)\n",
            "\n",
            "8375          climat scienc settl sustain stagnant\n",
            "6110          call climat plan good start joebiden\n",
            "16330    obama govern need save planet global warm\n",
            "Name: text, dtype: object (3602,)\n",
            "\n",
            "3641     0\n",
            "11837    1\n",
            "8450     1\n",
            "Name: label, dtype: int64 (14407,)\n",
            "\n",
            "8375     1\n",
            "6110     0\n",
            "16330    1\n",
            "Name: label, dtype: int64 (3602,)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIZniVph05_A",
        "colab_type": "text"
      },
      "source": [
        "## **Logistic Regression**\n",
        "\n",
        "- Logistic Regression is the appropriate regression analysis to conduct when the dependent variable is binary \n",
        "- From the results shown later we can see that the results given by different text representations doesn-t really differ. However the best results are given by the Bag of Words representation using Bigrams.\n",
        "\n",
        "Hyper-parameters : \n",
        "- penalty : {l1, l2, elasticnet} (The norm used in penalization)\n",
        "- solver : {'liblinear, 'saga', lbfgs} (Algorithm use in the optimization problem)\n",
        "- l1_ratio : float (Only in the case of elasticnet)\n",
        "- multi_class : SET TO 'ovr'\n",
        "- max_iter : SET TO 1000\n",
        "- C: float (Results showed that the best one is the default one (C=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHlvCgPY1EhQ",
        "colab_type": "text"
      },
      "source": [
        "# **Word Vectorisations**\n",
        "\n",
        "- Bag of Words (BOW) Binary (\"One-Hot\") Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VmiyDFG03go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bag of Words Representation (One Hot, i.e. binary)\n",
        "\n",
        "BOW_vectorizer = CountVectorizer(stop_words = 'english', \n",
        "                                 binary=True, # Creates 0/1 \"One Hot\" vector; \n",
        "                                              # np.unique(BOW_train.toarray())\n",
        "                                 min_df = 10)\n",
        "BOW_vectorizer.fit(X_train)\n",
        "BOW_train = BOW_vectorizer.transform(X_train)\n",
        "BOW_test = BOW_vectorizer.transform(X_test)\n",
        "\n",
        "# Construct Scaled Datasets\n",
        "\n",
        "scaler_BOW = MaxAbsScaler()\n",
        "BOW_train_scaled = scaler_BOW.fit_transform(BOW_train)\n",
        "BOW_test_scaled = scaler_BOW.transform(BOW_test)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC9IBo2C1NRC",
        "colab_type": "text"
      },
      "source": [
        "- Bag of Words with Frequencies Representation (FBOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pN7zXvD1Mea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bag of Words Representation (Frequencies; binary=False)\n",
        "\n",
        "FBOW_vectorizer = CountVectorizer(stop_words = 'english', \n",
        "                                  binary=False, # Creates Word Frequency Vector; \n",
        "                                                # # np.unique(FBOW_train.toarray())\n",
        "                                  min_df = 10)\n",
        "FBOW_vectorizer.fit(X_train)\n",
        "FBOW_train = FBOW_vectorizer.transform(X_train)\n",
        "FBOW_test = FBOW_vectorizer.transform(X_test)\n",
        "\n",
        "# Construct Scaled Datasets\n",
        "\n",
        "scaler_FBOW = MaxAbsScaler()\n",
        "FBOW_train_scaled = scaler_FBOW.fit_transform(FBOW_train)\n",
        "FBOW_test_scaled = scaler_FBOW.transform(FBOW_test)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFLRwUku1XD-",
        "colab_type": "text"
      },
      "source": [
        "- Bag of Words Bigrams (Bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1e4EGvx1WVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigram_vectorizer = CountVectorizer(stop_words = 'english', \n",
        "                                    binary=True, \n",
        "                                    min_df = 10,\n",
        "                                    ngram_range = (1,2)) # create bigrams\n",
        "bigram_vectorizer.fit(X_train)\n",
        "\n",
        "bigram_train = bigram_vectorizer.transform(X_train)\n",
        "bigram_test = bigram_vectorizer.transform(X_test)\n",
        "\n",
        "# Construct Scaled Datasets\n",
        "\n",
        "scaler_bigram = MaxAbsScaler()\n",
        "bigram_train_scaled = scaler_bigram.fit_transform(bigram_train)\n",
        "bigram_test_scaled = scaler_bigram.transform(bigram_test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8BfjkYv1h62",
        "colab_type": "text"
      },
      "source": [
        "- Term Frequency–Inverse Document Frequency Representation (tf-idf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KcIn1SE1dPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', \n",
        "                                   min_df=10) # used for now for consistency\n",
        "tfidf_vectorizer.fit(X_train)\n",
        "tfidf_train = tfidf_vectorizer.transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Construct Scaled Datasets\n",
        "\n",
        "scaler_tfidf = MaxAbsScaler()\n",
        "tfidf_train_scaled = scaler_tfidf.fit_transform(tfidf_train)\n",
        "tfidf_test_scaled = scaler_tfidf.transform(tfidf_test)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m15Srv3b1nDr",
        "colab_type": "text"
      },
      "source": [
        "# **Modelling** \n",
        "- Logistic Regression : BOW Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqYWPAEM1kYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "a13e1a7c-b37d-487a-88bd-582db014b36d"
      },
      "source": [
        "clf_BoW = LogisticRegression(C = 1, max_iter = 1000, multi_class = \"ovr\")\n",
        "clf_BoW.fit(BOW_train_scaled, Y_train)\n",
        "pred = clf_BoW.predict(BOW_test_scaled)\n",
        "accuracy = accuracy_score(Y_test,pred)\n",
        "precision = precision_score(Y_test,pred)\n",
        "recall = recall_score(Y_test,pred)\n",
        "f1 = f1_score(Y_test,pred)\n",
        "f2 = fbeta_score(Y_test,pred,beta=2)\n",
        "f5 = fbeta_score(Y_test,pred,beta=2)\n",
        "auc_score = roc_auc_score(Y_test, clf_BoW.predict_proba(BOW_test_scaled)[:, 1])\n",
        "\n",
        "print('-- Logistic Regression : BoW Scaled -- ')\n",
        "print()\n",
        "print('Accuracy: {}'.format(round(accuracy,4)))\n",
        "print('Precision: {}'.format(round(precision,4)))\n",
        "print('Recall: {}'.format(round(recall,4)))\n",
        "print('f1_score: {}'.format(round(f1,4)))\n",
        "print('f2_score: {}'.format(round(f2,4)))\n",
        "print('f5_score: {}'.format(round(f5,4)))\n",
        "print(\"Auc score : {}\".format(round(auc_score,4)))\n",
        "print()\n",
        "\n",
        "clf_BoW.fit(BOW_train, Y_train)\n",
        "pred = clf_BoW.predict(BOW_test)\n",
        "accuracy = accuracy_score(Y_test,pred)\n",
        "precision = precision_score(Y_test,pred)\n",
        "recall = recall_score(Y_test,pred)\n",
        "f1 = f1_score(Y_test,pred)\n",
        "f2 = fbeta_score(Y_test,pred,beta=2)\n",
        "f5 = fbeta_score(Y_test,pred,beta=2)\n",
        "auc_score = roc_auc_score(Y_test, clf_BoW.predict_proba(BOW_test)[:, 1])\n",
        "\n",
        "print('-- Logistic Regression : BoW Non Scaled -- ')\n",
        "print('Accuracy: {}'.format(round(accuracy,4)))\n",
        "print('Precision: {}'.format(round(precision,4)))\n",
        "print('Recall: {}'.format(round(recall,4)))\n",
        "print('f1_score: {}'.format(round(f1,4)))\n",
        "print('f2_score: {}'.format(round(f2,4)))\n",
        "print('f5_score: {}'.format(round(f5,4)))\n",
        "print(\"Auc score : {}\".format(round(auc_score,4)))\n",
        "print('--------------')\n",
        "print()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Logistic Regression : BoW Scaled -- \n",
            "\n",
            "Accuracy: 0.9098\n",
            "Precision: 0.9175\n",
            "Recall: 0.9317\n",
            "f1_score: 0.9246\n",
            "f2_score: 0.9288\n",
            "f5_score: 0.9288\n",
            "Auc score : 0.9687\n",
            "--------------\n",
            "\n",
            "-- Logistic Regression : BoW Non Scaled -- \n",
            "Accuracy: 0.9098\n",
            "Precision: 0.9175\n",
            "Recall: 0.9317\n",
            "f1_score: 0.9246\n",
            "f2_score: 0.9288\n",
            "f5_score: 0.9288\n",
            "Auc score : 0.9687\n",
            "--------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rvOqvEA2oom",
        "colab_type": "text"
      },
      "source": [
        "- Logistic Regression : Bigrams Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BZrjCjh2ddH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "f98bc102-e039-454c-9cc0-d5f72e5cf9e2"
      },
      "source": [
        "clf_BBoW = LogisticRegression(C = 1, max_iter = 1000, multi_class = \"ovr\", solver = \"saga\")\n",
        "clf_BBoW.fit(bigram_train_scaled, Y_train)\n",
        "pred = clf_BBoW.predict(bigram_test_scaled)\n",
        "accuracy = accuracy_score(Y_test,pred)\n",
        "precision = precision_score(Y_test,pred)\n",
        "recall = recall_score(Y_test,pred)\n",
        "f1 = f1_score(Y_test,pred)\n",
        "f2 = fbeta_score(Y_test,pred,beta=2)\n",
        "f5 = fbeta_score(Y_test,pred,beta=2)\n",
        "auc_score = roc_auc_score(Y_test, clf_BBoW.predict_proba(bigram_test_scaled)[:, 1])\n",
        "\n",
        "print('-- Logistic Regression : Bigrams Scaled -- ')\n",
        "print()\n",
        "print('Accuracy: {}'.format(round(accuracy,4)))\n",
        "print('Precision: {}'.format(round(precision,4)))\n",
        "print('Recall: {}'.format(round(recall,4)))\n",
        "print('f1_score: {}'.format(round(f1,4)))\n",
        "print('f2_score: {}'.format(round(f2,4)))\n",
        "print('f5_score: {}'.format(round(f5,4)))\n",
        "print(\"Auc score : {}\".format(round(auc_score,4)))\n",
        "print()\n",
        "\n",
        "clf_BBoW.fit(bigram_train, Y_train)\n",
        "pred = clf_BBoW.predict(bigram_test)\n",
        "accuracy = accuracy_score(Y_test,pred)\n",
        "precision = precision_score(Y_test,pred)\n",
        "recall = recall_score(Y_test,pred)\n",
        "f1 = f1_score(Y_test,pred)\n",
        "f2 = fbeta_score(Y_test,pred,beta=2)\n",
        "f5 = fbeta_score(Y_test,pred,beta=2)\n",
        "auc_score = roc_auc_score(Y_test, clf_BBoW.predict_proba(bigram_test)[:, 1])\n",
        "\n",
        "print('-- Logistic Regression : Bigrams Non Scaled -- ')\n",
        "print('Accuracy: {}'.format(round(accuracy,4)))\n",
        "print('Precision: {}'.format(round(precision,4)))\n",
        "print('Recall: {}'.format(round(recall,4)))\n",
        "print('f1_score: {}'.format(round(f1,4)))\n",
        "print('f2_score: {}'.format(round(f2,4)))\n",
        "print('f5_score: {}'.format(round(f5,4)))\n",
        "print(\"Auc score : {}\".format(round(auc_score,4)))\n",
        "print('--------------')\n",
        "print()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Logistic Regression : Bigrams Scaled -- \n",
            "\n",
            "Accuracy: 0.9203\n",
            "Precision: 0.9267\n",
            "Recall: 0.9401\n",
            "f1_score: 0.9334\n",
            "f2_score: 0.9374\n",
            "f5_score: 0.9374\n",
            "Auc score : 0.9723\n",
            "\n",
            "-- Logistic Regression : Bigrams Non Scaled -- \n",
            "Accuracy: 0.9203\n",
            "Precision: 0.9267\n",
            "Recall: 0.9401\n",
            "f1_score: 0.9334\n",
            "f2_score: 0.9374\n",
            "f5_score: 0.9374\n",
            "Auc score : 0.9723\n",
            "--------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPmXMp-93LKK",
        "colab_type": "text"
      },
      "source": [
        "- Logistic Regression : FBOW Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyA61BBP3KeP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "40faa4d3-070a-4b4f-8e2f-fb0eb1d1d93b"
      },
      "source": [
        "clf_FBoW = LogisticRegression(C = 1, max_iter = 1000, multi_class = \"ovr\", solver = \"liblinear\")\n",
        "clf_FBoW.fit(FBOW_train_scaled, Y_train)\n",
        "pred = clf_FBoW.predict(FBOW_test_scaled)\n",
        "accuracy = accuracy_score(Y_test,pred)\n",
        "precision = precision_score(Y_test,pred)\n",
        "recall = recall_score(Y_test,pred)\n",
        "f1 = f1_score(Y_test,pred)\n",
        "f2 = fbeta_score(Y_test,pred,beta=2)\n",
        "f5 = fbeta_score(Y_test,pred,beta=2)\n",
        "auc_score = roc_auc_score(Y_test, clf_FBoW.predict_proba(FBOW_test_scaled)[:, 1])\n",
        "\n",
        "print('-- Logistic Regression : FBoW Scaled -- ')\n",
        "print()\n",
        "print('Accuracy: {}'.format(round(accuracy,4)))\n",
        "print('Precision: {}'.format(round(precision,4)))\n",
        "print('Recall: {}'.format(round(recall,4)))\n",
        "print('f1_score: {}'.format(round(f1,4)))\n",
        "print('f2_score: {}'.format(round(f2,4)))\n",
        "print('f5_score: {}'.format(round(f5,4)))\n",
        "print(\"Auc score : {}\".format(round(auc_score,4)))\n",
        "print()\n",
        "\n",
        "clf_FBoW.fit(FBOW_train, Y_train)\n",
        "pred = clf_FBoW.predict(FBOW_test)\n",
        "accuracy = accuracy_score(Y_test,pred)\n",
        "precision = precision_score(Y_test,pred)\n",
        "recall = recall_score(Y_test,pred)\n",
        "f1 = f1_score(Y_test,pred)\n",
        "f2 = fbeta_score(Y_test,pred,beta=2)\n",
        "f5 = fbeta_score(Y_test,pred,beta=2)\n",
        "auc_score = roc_auc_score(Y_test, clf_FBoW.predict_proba(FBOW_test)[:, 1])\n",
        "\n",
        "print('-- Logistic Regression : FBoW Non Scaled -- ')\n",
        "print('Accuracy: {}'.format(round(accuracy,4)))\n",
        "print('Precision: {}'.format(round(precision,4)))\n",
        "print('Recall: {}'.format(round(recall,4)))\n",
        "print('f1_score: {}'.format(round(f1,4)))\n",
        "print('f2_score: {}'.format(round(f2,4)))\n",
        "print('f5_score: {}'.format(round(f5,4)))\n",
        "print(\"Auc score : {}\".format(round(auc_score,4)))\n",
        "print('--------------')\n",
        "print()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Logistic Regression : FBoW Scaled -- \n",
            "\n",
            "Accuracy: 0.9117\n",
            "Precision: 0.9201\n",
            "Recall: 0.9322\n",
            "f1_score: 0.9261\n",
            "f2_score: 0.9297\n",
            "f5_score: 0.9297\n",
            "Auc score : 0.9702\n",
            "\n",
            "-- Logistic Regression : FBoW Non Scaled -- \n",
            "Accuracy: 0.91\n",
            "Precision: 0.9187\n",
            "Recall: 0.9308\n",
            "f1_score: 0.9247\n",
            "f2_score: 0.9283\n",
            "f5_score: 0.9283\n",
            "Auc score : 0.969\n",
            "--------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C0hLLt23zu9",
        "colab_type": "text"
      },
      "source": [
        "- Logistic Regression : TF-IDF Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRRxM7N43qSL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "d5d4e98a-00fc-4be8-a194-6d5a64c87aa3"
      },
      "source": [
        "clf_TFIDF = LogisticRegression(C = 1, l1_ratio = 0.2,  penalty = \"elasticnet\", max_iter = 1000, multi_class = \"ovr\", solver = \"saga\")\n",
        "clf_TFIDF.fit(tfidf_train_scaled, Y_train)\n",
        "pred = clf_TFIDF.predict(tfidf_test_scaled)\n",
        "accuracy = accuracy_score(Y_test,pred)\n",
        "precision = precision_score(Y_test,pred)\n",
        "recall = recall_score(Y_test,pred)\n",
        "f1 = f1_score(Y_test,pred)\n",
        "f2 = fbeta_score(Y_test,pred,beta=2)\n",
        "f5 = fbeta_score(Y_test,pred,beta=2)\n",
        "auc_score = roc_auc_score(Y_test, clf_TFIDF.predict_proba(tfidf_test_scaled)[:, 1])\n",
        "\n",
        "print('-- Logistic Regression : TF-IDF Scaled -- ')\n",
        "print()\n",
        "print('Accuracy: {}'.format(round(accuracy,4)))\n",
        "print('Precision: {}'.format(round(precision,4)))\n",
        "print('Recall: {}'.format(round(recall,4)))\n",
        "print('f1_score: {}'.format(round(f1,4)))\n",
        "print('f2_score: {}'.format(round(f2,4)))\n",
        "print('f5_score: {}'.format(round(f5,4)))\n",
        "print(\"Auc score : {}\".format(round(auc_score,4)))\n",
        "print()\n",
        "\n",
        "clf_TFIDF.fit(tfidf_train, Y_train)\n",
        "pred = clf_TFIDF.predict(tfidf_test)\n",
        "accuracy = accuracy_score(Y_test,pred)\n",
        "precision = precision_score(Y_test,pred)\n",
        "recall = recall_score(Y_test,pred)\n",
        "f1 = f1_score(Y_test,pred)\n",
        "f2 = fbeta_score(Y_test,pred,beta=2)\n",
        "f5 = fbeta_score(Y_test,pred,beta=2)\n",
        "auc_score = roc_auc_score(Y_test, clf_TFIDF.predict_proba(tfidf_test)[:, 1])\n",
        "\n",
        "print('-- Logistic Regression : TF-IDF Non Scaled -- ')\n",
        "print('Accuracy: {}'.format(round(accuracy,4)))\n",
        "print('Precision: {}'.format(round(precision,4)))\n",
        "print('Recall: {}'.format(round(recall,4)))\n",
        "print('f1_score: {}'.format(round(f1,4)))\n",
        "print('f2_score: {}'.format(round(f2,4)))\n",
        "print('f5_score: {}'.format(round(f5,4)))\n",
        "print(\"Auc score : {}\".format(round(auc_score,4)))\n",
        "print('--------------')\n",
        "print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Logistic Regression : TF-IDF Scaled -- \n",
            "\n",
            "Accuracy: 0.9153\n",
            "Precision: 0.9257\n",
            "Recall: 0.9322\n",
            "f1_score: 0.9289\n",
            "f2_score: 0.9309\n",
            "f5_score: 0.9309\n",
            "Auc score : 0.9702\n",
            "\n",
            "-- Logistic Regression : TF-IDF Non Scaled -- \n",
            "Accuracy: 0.9134\n",
            "Precision: 0.9243\n",
            "Recall: 0.9303\n",
            "f1_score: 0.9273\n",
            "f2_score: 0.9291\n",
            "f5_score: 0.9291\n",
            "Auc score : 0.9686\n",
            "--------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}