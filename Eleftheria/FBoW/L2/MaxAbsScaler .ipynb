{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82aOeuGJG-el"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3489,
     "status": "ok",
     "timestamp": 1591971060899,
     "user": {
      "displayName": "Eleftheria Ttl",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjEUn7VOujIAs9GyA_K_R9ukpn5Xo-J__z4rTCB=s64",
      "userId": "04117230757505482464"
     },
     "user_tz": -120
    },
    "id": "HoTPbaY01FnW",
    "outputId": "a99772a3-b2d6-421d-a44d-823b4214a66a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eleftheria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv  # for slang\n",
    "import os\n",
    "import re  # regex\n",
    "import string  # punct\n",
    "from pprint import pprint\n",
    "#!pip install emoji --upgrade\n",
    "import emoji  # for emoji\n",
    "import gensim\n",
    "import keras\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from gensim.models import Word2Vec\n",
    "from IPython.display import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords  # stopwords\n",
    "from nltk.stem import PorterStemmer  # stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import svm, tree\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              RandomForestClassifier, RandomForestRegressor)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import (accuracy_score, auc, average_precision_score,\n",
    "                             brier_score_loss, classification_report,\n",
    "                             confusion_matrix, f1_score, fbeta_score,\n",
    "                             make_scorer,\n",
    "                             precision_recall_curve, precision_score,\n",
    "                             recall_score, roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, RandomizedSearchCV,\n",
    "                                     cross_val_score, train_test_split)\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wM0GLzDG1cZo"
   },
   "source": [
    "# Homemade Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vKjPveFV1XAZ"
   },
   "outputs": [],
   "source": [
    "# Clean Text Class\n",
    "\n",
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def remove_mentions(self, input_text):\n",
    "        '''\n",
    "        Remove mentions, like @Mplamplampla\n",
    "        '''\n",
    "        return re.sub(r'@+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        '''\n",
    "        Remove the urls mention in a tweet\n",
    "        '''\n",
    "        input_text  = ' '.join([w for w in input_text.split(' ') if '.com' not in w])\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        input_text = emoji.demojize(input_text)\n",
    "        input_text = input_text.replace('_','')\n",
    "        input_text = input_text.replace(':','')\n",
    "        return input_text\n",
    "    \n",
    "    def possessive_pronouns(self, input_text):\n",
    "        '''\n",
    "        Remove the possesive pronouns, because otherwise after tokenization we will end up with a word and an s\n",
    "        Example: government's --> [\"government\", \"s\"]\n",
    "        '''\n",
    "        return input_text.replace(\"'s\", \"\")\n",
    "    \n",
    "    def characters(self, input_text):\n",
    "        '''\n",
    "        Remove special and redundant characters that may appear on a tweet and that don't really help in our analysis\n",
    "        '''\n",
    "        input_text = input_text.replace(\"\\r\", \" \") # Carriage Return\n",
    "        input_text = input_text.replace(\"\\n\", \" \") # Newline\n",
    "        input_text = \" \".join(input_text.split()) # Double space\n",
    "        input_text = input_text.replace('\"', '') # Quotes\n",
    "        return input_text\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        '''\n",
    "        Remove punctuation and specifically these symbols '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        '''\n",
    "        punct = string.punctuation # string with all the punctuation symbols '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    \n",
    "    def remove_digits(self, input_text):\n",
    "        '''\n",
    "        Remove numbers\n",
    "        '''\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        '''\n",
    "        Convert all the sentences(words) to lowercase\n",
    "        '''\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        '''\n",
    "        Remove stopwords (refers to the most common words in a language)\n",
    "        '''\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def stemming(self, input_text):\n",
    "        '''\n",
    "        Reduce the words to their stem\n",
    "        '''\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def encode_decode(self, input_text):\n",
    "        '''\n",
    "        Remove weird characters that are result of encoding problems\n",
    "        '''\n",
    "        return  \" \".join([k.encode(\"ascii\", \"ignore\").decode() for k in input_text.split(\" \")])\n",
    "    \n",
    "    \n",
    "    def translator(self, input_text):\n",
    "        '''\n",
    "        Transform abbrevations to normal words\n",
    "        Example: asap --> as soon as possible\n",
    "        '''\n",
    "        input_text = input_text.split(\" \")\n",
    "        j = 0\n",
    "        for _str in input_text:\n",
    "            # File path which consists of Abbreviations.\n",
    "            fileName = r\"C:\\Users\\Eleftheria\\Desktop\\slang.txt\"\n",
    "            # File Access mode [Read Mode]\n",
    "            accessMode = \"r\"\n",
    "            with open(fileName, accessMode) as myCSVfile:\n",
    "                # Reading file as CSV with delimiter as \"=\", so that \n",
    "                # abbreviation are stored in row[0] and phrases in row[1]\n",
    "                dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "                # Removing Special Characters.\n",
    "                _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "                for row in dataFromFile:\n",
    "                    # Check if selected word matches short forms[LHS] in text file.\n",
    "                    if _str.upper() == row[0]:\n",
    "                        # If match found replace it with its appropriate phrase in text file.\n",
    "                        input_text[j] = row[1]\n",
    "                myCSVfile.close()\n",
    "            j = j + 1\n",
    "        \n",
    "        return(' '.join(input_text))\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = (X.apply(self.translator)\n",
    "                    .apply(self.remove_mentions)\n",
    "                    .apply(self.remove_urls)\n",
    "                    .apply(self.emoji_oneword)\n",
    "                    .apply(self.possessive_pronouns)\n",
    "                    .apply(self.remove_punctuation)\n",
    "                    .apply(self.remove_digits)\n",
    "                    .apply(self.encode_decode)\n",
    "                    .apply(self.characters)\n",
    "                    .apply(self.to_lower)\n",
    "                    .apply(self.remove_stopwords)\n",
    "                    .apply(self.stemming))\n",
    "        return clean_X\n",
    "    \n",
    "    def transform_no_stem(self, X, **transform_params):\n",
    "        clean_X = (X.apply(self.translator)\n",
    "                    .apply(self.remove_mentions)\n",
    "                    .apply(self.remove_urls)\n",
    "                    .apply(self.emoji_oneword)\n",
    "                    .apply(self.possessive_pronouns)\n",
    "                    .apply(self.remove_punctuation)\n",
    "                    .apply(self.remove_digits)\n",
    "                    .apply(self.encode_decode)\n",
    "                    .apply(self.characters)\n",
    "                    .apply(self.to_lower)\n",
    "                    .apply(self.remove_stopwords))\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oX7yLqQg16hO"
   },
   "source": [
    "# Read in Data and Create Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2230,
     "status": "ok",
     "timestamp": 1591971070839,
     "user": {
      "displayName": "Eleftheria Ttl",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjEUn7VOujIAs9GyA_K_R9ukpn5Xo-J__z4rTCB=s64",
      "userId": "04117230757505482464"
     },
     "user_tz": -120
    },
    "id": "ddQSI3QK16BE",
    "outputId": "b731bd19-192c-4e5e-eb99-5c81aa5bc554"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>WWF Climate &amp; Energy</td>\n",
       "      <td>climateWWF</td>\n",
       "      <td>2020-04-28</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>Economic recovery and national climate pledges...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>WWF Climate &amp; Energy</td>\n",
       "      <td>climateWWF</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>In this difficult time, it’s hard to connect w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>WWF Climate &amp; Energy</td>\n",
       "      <td>climateWWF</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>43</td>\n",
       "      <td>69</td>\n",
       "      <td>The decision to postpone # COP26, is unavoidab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>WWF Climate &amp; Energy</td>\n",
       "      <td>climateWWF</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>Japan - the world’s fifth largest emitter of g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>WWF Climate &amp; Energy</td>\n",
       "      <td>climateWWF</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>22</td>\n",
       "      <td>40</td>\n",
       "      <td>How can countries include # NatureBasedSolutio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               username user_handle        date retweets favorites                                               text  label\n",
       "0  WWF Climate & Energy  climateWWF  2020-04-28       11        22  Economic recovery and national climate pledges...      0\n",
       "1  WWF Climate & Energy  climateWWF  2020-04-22        6        16  In this difficult time, it’s hard to connect w...      0\n",
       "2  WWF Climate & Energy  climateWWF  2020-04-01       43        69  The decision to postpone # COP26, is unavoidab...      0\n",
       "3  WWF Climate & Energy  climateWWF  2020-03-30       24        30  Japan - the world’s fifth largest emitter of g...      0\n",
       "4  WWF Climate & Energy  climateWWF  2020-03-30       22        40  How can countries include # NatureBasedSolutio...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('https://github.com/anilkeshwani/StatLearnProj/raw/master/Iason/climate_change_tweets_sample-2020-05-16-17-57.csv')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1UTJwqm52Nhi"
   },
   "source": [
    "# Text Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOri2tIV2NOy"
   },
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "tweets[\"text\"] = ct.fit_transform(tweets.text)\n",
    "tweets.to_csv(\"clean_tweets.csv\") # save once processed\n",
    "tweets = pd.read_csv(\"clean_tweets.csv\") # read in instead\n",
    "tweets = tweets.loc[(~tweets.text.isnull()), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lI3OIAs5YJlw"
   },
   "source": [
    "# Splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 959,
     "status": "ok",
     "timestamp": 1591971165942,
     "user": {
      "displayName": "Eleftheria Ttl",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjEUn7VOujIAs9GyA_K_R9ukpn5Xo-J__z4rTCB=s64",
      "userId": "04117230757505482464"
     },
     "user_tz": -120
    },
    "id": "S6BWphtRYMdO",
    "outputId": "a21164fe-6116-4cd2-ff8a-3f71a0aac9c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3642     might progress climat area not enough global s...\n",
      "12695    trump crackdown politic scienc nasa climat div...\n",
      "8451     no one would believ human panick fals climat m...\n",
      "Name: text, dtype: object (14406,)\n",
      "\n",
      "8376     nazi root environment climat chang fraud bbcne...\n",
      "6111     interest democrat candid compar mani aspect cl...\n",
      "13983    ittrademark imposs see global warm signal minn...\n",
      "Name: text, dtype: object (3602,)\n",
      "\n",
      "3642     0\n",
      "12695    1\n",
      "8451     1\n",
      "Name: label, dtype: int64 (14406,)\n",
      "\n",
      "8376     1\n",
      "6111     0\n",
      "13983    1\n",
      "Name: label, dtype: int64 (3602,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(tweets.text, tweets.label, \n",
    "                                                    test_size=0.2, random_state=17, \n",
    "                                                    shuffle=True) # explicit default\n",
    "\n",
    "[print(dat.head(3), dat.shape, end=\"\\n\"*2) for dat in [X_train, X_test, Y_train, Y_test]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1591971182851,
     "user": {
      "displayName": "Eleftheria Ttl",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjEUn7VOujIAs9GyA_K_R9ukpn5Xo-J__z4rTCB=s64",
      "userId": "04117230757505482464"
     },
     "user_tz": -120
    },
    "id": "NPRWPVbI4G1X",
    "outputId": "789e1121-1500-48ee-fa17-621bb028f2eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label counts: \n",
      "1    8433\n",
      "0    5973\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Test label counts: \n",
      "1    2138\n",
      "0    1464\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training label counts: \\n{Y_train.value_counts()}\", end=\"\\n\"*2)\n",
    "print(f\"Test label counts: \\n{Y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3HdRAth9YQxX"
   },
   "source": [
    "# Word Vectorisations \n",
    "FBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j5hnkhAuYZt-"
   },
   "outputs": [],
   "source": [
    "FBOW_vectorizer = CountVectorizer(stop_words = 'english', \n",
    "                                  binary=False, # Creates Word Frequency Vector; \n",
    "                                                # # np.unique(FBOW_train.toarray())\n",
    "                                  min_df = 10)\n",
    "FBOW_vectorizer.fit(X_train)\n",
    "X_train = FBOW_vectorizer.transform(X_train)\n",
    "X_test = FBOW_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GRcmYvfIYgcw"
   },
   "source": [
    "# Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fQG02_dYhxK"
   },
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zViBspW8Yl05"
   },
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xA1X4y2Yki5"
   },
   "outputs": [],
   "source": [
    "kfcv = KFold(n_splits=5,shuffle=True,random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TAniklhnYp25"
   },
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "reyAVwTuYsDw"
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "            'penalty': ['l2'],\n",
    "            'C': [.001, .01, .1, 1, 10, 100, 1000],\n",
    "            'solver': ['liblinear',\"saga\", \"lbfgs\", \"newton-cg\", \"sag\"],\n",
    "            'multi_class': ['ovr'],\n",
    "            'max_iter' : [1000]\n",
    "        }\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-jiSAi-uYrcC"
   },
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 99342,
     "status": "ok",
     "timestamp": 1591971905752,
     "user": {
      "displayName": "Eleftheria Ttl",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjEUn7VOujIAs9GyA_K_R9ukpn5Xo-J__z4rTCB=s64",
      "userId": "04117230757505482464"
     },
     "user_tz": -120
    },
    "id": "wVhq1bGEYroD",
    "outputId": "2f917798-f34b-4221-9465-6d4da073969b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   28.1s\n",
      "[Parallel(n_jobs=-1)]: Done 175 out of 175 | elapsed:   43.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=101, shuffle=True),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
       "                         'max_iter': [1000], 'multi_class': ['ovr'],\n",
       "                         'penalty': ['l2'],\n",
       "                         'solver': ['liblinear', 'saga', 'lbfgs', 'newton-cg',\n",
       "                                    'sag']},\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator = clf, param_grid = param_grid, \n",
    "                           cv=kfcv,n_jobs = -1, verbose =2, scoring='accuracy')\n",
    "grid_search.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 836,
     "status": "ok",
     "timestamp": 1591971920227,
     "user": {
      "displayName": "Eleftheria Ttl",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjEUn7VOujIAs9GyA_K_R9ukpn5Xo-J__z4rTCB=s64",
      "userId": "04117230757505482464"
     },
     "user_tz": -120
    },
    "id": "9_UOQcv1Hoc3",
    "outputId": "bbc4c5cc-97b1-4fa4-af9f-d27c328f65dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data\n",
      "\n",
      "LogisticRegression(C=1, max_iter=1000, multi_class='ovr', solver='liblinear')\n",
      "\n",
      "Score of best_estimator on the left out data.\n",
      "\n",
      "0.907\n",
      "\n",
      "\n",
      "\n",
      "Scorer function used on the held out data to choose the best parameters for the model.\n",
      "\n",
      "make_scorer(accuracy_score)\n"
     ]
    }
   ],
   "source": [
    "#print(\"Contains scores for all parameter combinations in param_grid. Each entry corresponds to one parameter setting. Each named tuple has the attributes: \\n--> parameters : a dict of parameter settings \\n--> mean_validation_score : the mean score over the cross-validation folds \\n--> cv_validation_scores : the list of scores for each fold\")\n",
    "#print()\n",
    "#print(grid_search.cv_results_)\n",
    "#print()\n",
    "print(\"Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data\")\n",
    "print()\n",
    "print(grid_search.best_estimator_)\n",
    "print()\n",
    "print(\"Score of best_estimator on the left out data.\")\n",
    "print()\n",
    "print(round(grid_search.best_score_,3) )\n",
    "print()\n",
    "#print(\"Parameter setting that gave the best results on the hold out data.\")\n",
    "print()\n",
    "#print(grid_search.best_estimator_.get_params)\n",
    "print()\n",
    "print(\"Scorer function used on the held out data to choose the best parameters for the model.\")\n",
    "print()\n",
    "print(grid_search.scorer_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 802,
     "status": "ok",
     "timestamp": 1591972383295,
     "user": {
      "displayName": "Eleftheria Ttl",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjEUn7VOujIAs9GyA_K_R9ukpn5Xo-J__z4rTCB=s64",
      "userId": "04117230757505482464"
     },
     "user_tz": -120
    },
    "id": "HAhJ4GJ6ba4S",
    "outputId": "4ecbe0a0-71e6-4c8f-9d07-ec17718c6708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mean Accuracy: 1.0\n",
      "Test Mean Accuracy : 0.907\n"
     ]
    }
   ],
   "source": [
    "print('Training Mean Accuracy: {}'.format(round(grid_search.best_estimator_.score(X_train, Y_train))))\n",
    "\n",
    "print(\"Test Mean Accuracy : {:.3f}\".format(grid_search.best_estimator_.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1591972531975,
     "user": {
      "displayName": "Eleftheria Ttl",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjEUn7VOujIAs9GyA_K_R9ukpn5Xo-J__z4rTCB=s64",
      "userId": "04117230757505482464"
     },
     "user_tz": -120
    },
    "id": "qQOSU-ac82q9",
    "outputId": "e9c296cc-0789-4076-a426-ad5e422d321f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (1 - Brier Score): 0.952\n",
      "Test (1 - Brier Score): 0.931\n"
     ]
    }
   ],
   "source": [
    "proba_train = grid_search.predict_proba(X_train)[:, 1]\n",
    "proba_test = grid_search.predict_proba(X_test)[:, 1]\n",
    "BOW_logreg_noreg_brier_train = brier_score_loss(Y_train, proba_train)\n",
    "BOW_logreg_noreg_brier_test = brier_score_loss(Y_test, proba_test)\n",
    "print(f\"Training (1 - Brier Score): {1 - round(BOW_logreg_noreg_brier_train, 3)}\")\n",
    "print(f\"Test (1 - Brier Score): {1 - round(BOW_logreg_noreg_brier_test, 3)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPyHmzJf3bKxQNLqpmdtvSt",
   "collapsed_sections": [],
   "name": "MaxAbsScaler .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
