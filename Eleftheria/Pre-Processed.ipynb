{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import string # punct\n",
    "import emoji # for emoji\n",
    "import csv # for slang\n",
    "import re # regex\n",
    "from nltk.corpus import stopwords # stopwords\n",
    "from nltk.stem import PorterStemmer # stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\HP\\Documents\\Year I Semester II\\SL\\Project\\Group 7 Pilot Dataset.csv\")\n",
    "df = df[['text', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def remove_mentions(self, input_text):\n",
    "        '''\n",
    "        Remove mentions, like @Mplamplampla\n",
    "        '''\n",
    "        return re.sub(r'@+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        '''\n",
    "        Remove the urls mention in a tweet\n",
    "        '''\n",
    "        input_text  = ' '.join([w for w in input_text.split(' ') if '.com' not in w])\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        input_text = emoji.demojize(input_text)\n",
    "        input_text = input_text.replace('_','')\n",
    "        input_text = input_text.replace(':','')\n",
    "        return input_text\n",
    "    \n",
    "    def possessive_pronouns(self, input_text):\n",
    "        '''\n",
    "        Remove the possesive pronouns, because otherwise after tokenization we will end up with a word and an s\n",
    "        Example: government's --> [\"government\", \"s\"]\n",
    "        '''\n",
    "        return input_text.replace(\"'s\", \"\")\n",
    "    \n",
    "    def characters(self, input_text):\n",
    "        '''\n",
    "        Remove special and redundant characters that may appear on a tweet and that don't really help in our analysis\n",
    "        '''\n",
    "        input_text = input_text.replace(\"\\r\", \" \") # Carriage Return\n",
    "        input_text = input_text.replace(\"\\n\", \" \") # Newline\n",
    "        input_text = \" \".join(input_text.split()) # Double space\n",
    "        input_text = input_text.replace('\"', '') # Quotes\n",
    "        return input_text\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        '''\n",
    "        Remove punctuation and specifically these symbols '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        '''\n",
    "        punct = string.punctuation # string with all the punctuation symbols '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    \n",
    "    def remove_digits(self, input_text):\n",
    "        '''\n",
    "        Remove numbers\n",
    "        '''\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        '''\n",
    "        Convert all the sentences(words) to lowercase\n",
    "        '''\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        '''\n",
    "        Remove stopwords (refers to the most common words in a language)\n",
    "        '''\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def stemming(self, input_text):\n",
    "        '''\n",
    "        Reduce the words to their stem\n",
    "        '''\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def encode_decode(self, input_text):\n",
    "        '''\n",
    "        Remove weird characters that are result of encoding problems\n",
    "        '''\n",
    "        return  \" \".join([k.encode(\"ascii\", \"ignore\").decode() for k in input_text.split(\" \")])\n",
    "    \n",
    "    \n",
    "    def translator(self, input_text):\n",
    "        '''\n",
    "        Transform abbrevations to normal words\n",
    "        Example: asap --> as soon as possible\n",
    "        '''\n",
    "        input_text = input_text.split(\" \")\n",
    "        j = 0\n",
    "        for _str in input_text:\n",
    "            # File path which consists of Abbreviations.\n",
    "            fileName = r\"C:\\Users\\HP\\Documents\\Year I Semester II\\SL\\Project\\StatLearnProj-master\\Iason\\slang.txt\"\n",
    "            # File Access mode [Read Mode]\n",
    "            accessMode = \"r\"\n",
    "            with open(fileName, accessMode) as myCSVfile:\n",
    "                # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "                dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "                # Removing Special Characters.\n",
    "                _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "                for row in dataFromFile:\n",
    "                    # Check if selected word matches short forms[LHS] in text file.\n",
    "                    if _str.upper() == row[0]:\n",
    "                        # If match found replace it with its appropriate phrase in text file.\n",
    "                        input_text[j] = row[1]\n",
    "                myCSVfile.close()\n",
    "            j = j + 1\n",
    "        \n",
    "        return(' '.join(input_text))\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.translator).apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.possessive_pronouns).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.encode_decode).apply(self.characters).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just wanted to say @Aladdin, you're so cute ❤️. I would suggest you go to www.diamondintherough.com and fill an \n",
      "application there asap â€ !!! You are 100% the best guy for Jasmine's heart \n"
     ]
    }
   ],
   "source": [
    "text = \"I just wanted to say @Aladdin, you're so cute ❤️. I would suggest you go to www.diamondintherough.com and fill an \\napplication there asap â€ !!! You are 100% the best guy for Jasmine's heart \"\n",
    "print(text)\n",
    "yo = CleanText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's eliminate abreviations (here \"asap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just wanted to say @Aladdin, you're so cute ❤️. I would suggest you go to www.diamondintherough.com and fill an \n",
      "application there As Soon As Possible â€ !!! You are 100% the best guy for Jasmine's heart \n"
     ]
    }
   ],
   "source": [
    "text = yo.translator(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's eliminate mentions (here \"@Aladdin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just wanted to say Aladdin, you're so cute ❤️. I would suggest you go to www.diamondintherough.com and fill an \n",
      "application there As Soon As Possible â€ !!! You are 100% the best guy for Jasmine's heart \n"
     ]
    }
   ],
   "source": [
    "text = yo.remove_mentions(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's eliminate urls (here \"www.diamondintherough.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just wanted to say Aladdin, you're so cute ❤️. I would suggest you go to and fill an \n",
      "application there As Soon As Possible â€ !!! You are 100% the best guy for Jasmine's heart \n"
     ]
    }
   ],
   "source": [
    "text = yo.remove_urls(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's eliminate emojis (here \"❤️\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just wanted to say Aladdin, you're so cute redheart. I would suggest you go to and fill an \n",
      "application there As Soon As Possible â€ !!! You are 100% the best guy for Jasmine's heart \n"
     ]
    }
   ],
   "source": [
    "text = yo.emoji_oneword(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's eliminate possesive pronouns (here \"Jasmine's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just wanted to say Aladdin, you're so cute redheart. I would suggest you go to and fill an \n",
      "application there As Soon As Possible â€ !!! You are 100% the best guy for Jasmine heart \n"
     ]
    }
   ],
   "source": [
    "text = yo.possessive_pronouns(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's eliminate punctuation (here \",\", \"'\", \".\", \"!!!\", \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just wanted to say Aladdin  you re so cute redheart  I would suggest you go to and fill an \n",
      "application there As Soon As Possible â€     You are 100  the best guy for Jasmine heart \n"
     ]
    }
   ],
   "source": [
    "text = yo.remove_punctuation(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's eliminate digits (here \"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just wanted to say Aladdin  you re so cute redheart  I would suggest you go to and fill an \n",
      "application there As Soon As Possible â€     You are   the best guy for Jasmine heart \n"
     ]
    }
   ],
   "source": [
    "text = yo.remove_digits(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's eliminate ascii weirs characters (here \"â€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just wanted to say Aladdin  you re so cute redheart  I would suggest you go to and fill an \n",
      "application there As Soon As Possible      You are   the best guy for Jasmine heart \n"
     ]
    }
   ],
   "source": [
    "text = yo.encode_decode(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's eliminate special charachers (here \"\\n\", you can see that after the analysis the new line doesn't start with application. Also remove excess whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just wanted to say Aladdin you re so cute redheart I would suggest you go to and fill an application there As Soon As Possible You are the best guy for Jasmine heart\n"
     ]
    }
   ],
   "source": [
    "text = yo.characters(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's transform all the words to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i just wanted to say aladdin you re so cute redheart i would suggest you go to and fill an application there as soon as possible you are the best guy for jasmine heart\n"
     ]
    }
   ],
   "source": [
    "text = yo.to_lower(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's eliminate stopwords (here \"i\", \"just\", \"to\", \"you\", \"re\",\"so\", \"you\", \"and\", \"an\", \"there\", \"as\", \"are\", \"the\", \"for\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wanted say aladdin cute redheart would suggest go fill application soon possible best guy jasmine heart\n"
     ]
    }
   ],
   "source": [
    "text = yo.remove_stopwords(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's keep just the stem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "want say aladdin cute redheart would suggest go fill applic soon possibl best guy jasmin heart\n"
     ]
    }
   ],
   "source": [
    "text = yo.stemming(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the class on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "sr_clean = ct.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dopo_eda = pd.DataFrame()\n",
    "dopo_eda[\"text\"] = sr_clean\n",
    "dopo_eda[\"label\"] = df.label\n",
    "dopo_eda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A check, just in case after the pre-processing eliminated everything :p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_clean = dopo_eda.text == ''\n",
    "print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))\n",
    "print(\"Before\")\n",
    "print(df[empty_clean])\n",
    "print(\"After\")\n",
    "print(sr_clean[empty_clean])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since there's nothing left we'll drop them and reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dopo_eda.drop([2062, 3087], inplace = True)\n",
    "dopo_eda.reset_index(inplace = True)\n",
    "dopo_eda.drop(\"index\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dopo_eda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And now we save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dopo_eda.to_csv(\"dopo_eda.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
